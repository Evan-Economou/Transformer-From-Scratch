[MODEL]
d_model = 32
d_hidden = 128
num_blocks = 4

[DATA]
tokenizer_data_path = data/gutenberg_data/10.txt

[TRAINING]
batch_size=10000
positional_embedding=True
