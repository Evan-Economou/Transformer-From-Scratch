{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05314450",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "533bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../../../../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee444a7",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b63bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e41103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff83a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_qk = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_vo = nn.Linear(config.d_model, config.d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def create_mask(self, n_c: int) -> torch.Tensor:\n",
    "        mask: Float[torch.Tensor, \"seq_len seq_len\"] = torch.triu(-1 * torch.inf * torch.ones(n_c, n_c), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        #create mask, with size n_c x n_c\n",
    "        mask = self.create_mask(x.shape[0])\n",
    "\n",
    "        #compute attention scores\n",
    "        # A = softmax((X @ W_qk @ X^T) + M) @ X @ W_vo\n",
    "        A = self.softmax((self.W_qk(x)) @ x.transpose(0, -1) + mask) @ self.W_vo(x)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fd1570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attention_head = AttentionHead(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        return x + self.attention_head(x) + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "590a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Linear(config.d_vocab, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(num_blocks)])\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len vocab\"]) -> Float[torch.Tensor, \"vocab seq_len\"]:\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = (x @ self.embedding.weight).T\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0dc71",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a8979a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Attention head test\n",
    "x: Float[torch.Tensor, \"seq_len d_model\"] = torch.ones(5, 16)\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "attention_head: AttentionHead = AttentionHead(config)\n",
    "output: Float[torch.Tensor, \"seq_len d_model\"] = attention_head.forward(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d392391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor([ 6.3441e-03,  8.3942e-02, -3.9893e-02, -6.3548e-03,  3.3729e-02,\n",
      "        -2.8102e-02,  1.8724e-02,  3.9608e-03, -3.7850e-03, -3.1101e-02,\n",
      "         8.5074e-03,  6.8095e-02,  7.8028e-02,  5.5352e-02, -5.4220e-02,\n",
      "        -6.8182e-03,  8.6136e-02,  2.7536e-02,  1.8153e-02,  9.5712e-03,\n",
      "         2.0403e-02,  8.1410e-02,  5.7799e-02, -4.7979e-02, -8.2013e-02,\n",
      "        -1.7693e-02,  2.8380e-02, -1.7299e-02,  4.1108e-02,  2.5330e-02,\n",
      "         3.7915e-02, -1.5364e-02,  6.5139e-02, -1.2433e-02,  3.1411e-02,\n",
      "         3.7416e-02, -3.6107e-02,  1.8123e-02, -6.5640e-02,  4.5044e-02,\n",
      "        -1.0404e-02, -2.4668e-02,  7.0007e-02, -4.4044e-02,  6.9703e-02,\n",
      "         1.7621e-02, -2.2256e-02,  7.2073e-03, -2.0943e-02, -4.6131e-02,\n",
      "         4.8179e-02,  4.9247e-04, -5.3634e-02, -2.0521e-03,  5.7496e-02,\n",
      "         1.0118e-03, -1.6561e-02,  1.0509e-02,  4.7976e-03,  7.0115e-02,\n",
      "         9.9458e-03,  2.7358e-02,  5.3892e-02,  3.8064e-02,  9.3865e-02,\n",
      "        -3.7413e-02, -4.3165e-02, -1.3348e-02, -7.0005e-03,  1.3333e-03,\n",
      "         1.3486e-02, -3.3372e-02, -4.0390e-02,  5.8348e-02, -2.4435e-02,\n",
      "        -2.0101e-02,  4.8893e-03,  2.0663e-02, -3.4610e-02, -3.1519e-02,\n",
      "         2.0515e-02, -9.9995e-04, -3.1337e-02,  3.9214e-02,  1.6221e-02,\n",
      "        -2.9050e-02,  4.5410e-02, -4.3369e-02,  7.6155e-02,  2.9583e-02,\n",
      "         3.2034e-02, -2.8166e-03, -3.1619e-02, -3.1658e-02,  6.1081e-02,\n",
      "         1.6434e-02,  2.1658e-02, -1.5045e-02, -3.1284e-02,  1.7356e-02,\n",
      "        -1.8427e-02, -6.1511e-02,  1.0556e-04,  1.4944e-02, -9.9770e-03,\n",
      "         6.3837e-02,  1.7871e-02, -1.2346e-02, -1.7767e-02,  4.8081e-02,\n",
      "         3.2726e-03, -3.5107e-02,  1.6058e-02,  6.4319e-02, -2.3856e-03,\n",
      "        -5.4818e-03,  4.5212e-02,  1.5880e-02,  4.6584e-02, -1.8324e-02,\n",
      "         9.1837e-02,  3.0860e-02,  1.9223e-02,  1.7674e-02, -8.4300e-02,\n",
      "         3.2236e-02,  4.9653e-02, -5.6740e-02,  1.7235e-02, -1.8754e-02,\n",
      "        -4.6797e-03,  6.5746e-02,  6.9262e-03,  2.0649e-02,  1.7834e-02,\n",
      "         6.2086e-02,  2.5578e-03,  2.4492e-02,  4.6233e-02, -2.5239e-02,\n",
      "        -5.6470e-02,  9.1858e-03,  1.9545e-02, -6.8540e-02, -3.1356e-02,\n",
      "        -7.1496e-02,  1.5579e-02, -7.4997e-02,  8.8972e-02, -3.1450e-02,\n",
      "         6.3610e-02, -6.0485e-02, -6.0642e-02,  4.5905e-02,  2.5090e-02,\n",
      "        -2.2943e-02,  5.7345e-02,  3.8538e-02,  9.4255e-03, -7.5440e-02,\n",
      "         1.5859e-02, -5.8996e-03, -1.1687e-02, -3.9176e-02, -6.9703e-02,\n",
      "         1.6610e-02,  6.5568e-03, -4.0079e-03,  8.0895e-02, -3.7395e-02,\n",
      "        -4.9889e-02, -2.9027e-02, -2.8473e-02, -6.3915e-02, -1.6888e-03,\n",
      "        -8.1052e-03, -5.4797e-03,  2.7751e-02, -3.4403e-02, -9.9446e-03,\n",
      "         8.4818e-03,  5.1643e-02,  5.8475e-02, -4.3729e-02, -2.9021e-02,\n",
      "        -2.0939e-02,  1.1246e-02, -2.6664e-02,  1.1196e-02, -4.7191e-02,\n",
      "        -5.5792e-02, -4.8360e-02,  2.2441e-02, -5.8093e-02,  6.4313e-03,\n",
      "        -4.3387e-02,  2.1135e-02, -7.6165e-03, -3.8880e-02,  5.6668e-02,\n",
      "        -3.0345e-02, -4.0033e-02,  6.6493e-02,  3.2407e-02,  2.5760e-02,\n",
      "         3.3425e-02, -4.3642e-03, -3.5083e-02,  3.0424e-02, -3.3698e-02,\n",
      "         2.2979e-02, -4.0854e-02, -7.7874e-02,  5.1373e-02,  3.2839e-02,\n",
      "        -5.2343e-03,  1.4931e-02,  1.0401e-01,  1.0046e-01, -3.4786e-02,\n",
      "        -1.9894e-02,  7.9078e-03, -2.1424e-02, -2.8751e-02,  1.2052e-01,\n",
      "        -3.7813e-02, -2.6100e-02, -2.7572e-02, -4.8812e-02, -6.1292e-03,\n",
      "         2.4485e-03,  5.1517e-03,  6.6991e-02, -3.6081e-03, -3.3470e-02,\n",
      "        -1.1887e-02, -4.2650e-02,  5.0189e-02, -9.1923e-03,  2.2676e-02,\n",
      "         2.6217e-02,  1.7081e-02, -7.6976e-04, -2.3289e-02, -3.0350e-02,\n",
      "        -2.7732e-02, -6.0736e-02,  5.0778e-02,  3.8870e-02,  1.6074e-02,\n",
      "         6.6667e-02,  4.4506e-02, -9.3001e-02,  3.6503e-02, -2.6537e-02,\n",
      "        -4.1801e-02,  6.4052e-02, -6.2993e-04,  2.1133e-02,  1.8683e-02,\n",
      "        -4.2033e-02,  3.1233e-02,  2.6537e-02,  1.7993e-02,  6.3881e-02,\n",
      "        -8.1680e-02, -3.2364e-03,  5.6747e-03,  5.2298e-02,  4.9636e-02,\n",
      "         1.1599e-02, -1.5330e-02,  3.5453e-02, -2.4427e-02, -4.0836e-02,\n",
      "        -7.6014e-03,  6.4854e-02,  2.6235e-02, -2.7459e-04,  1.3508e-02,\n",
      "        -9.3927e-03,  3.5620e-02, -1.5192e-02, -8.2014e-02, -1.3854e-02,\n",
      "         5.3006e-02, -2.3804e-02, -5.2682e-03, -4.0674e-02, -1.7911e-02,\n",
      "        -6.2478e-02,  2.3656e-02,  4.0413e-03, -1.9406e-02,  2.3228e-02,\n",
      "        -2.6502e-02, -7.4917e-03,  6.8880e-03,  4.9977e-02,  2.1349e-02,\n",
      "         4.2144e-02,  6.4473e-02,  8.2755e-03, -6.2575e-02,  5.4519e-03,\n",
      "         2.1170e-02, -2.2786e-02,  7.4750e-02,  2.9585e-02, -2.1596e-02,\n",
      "        -1.3236e-02, -5.0098e-02, -1.5655e-02,  3.1232e-03, -1.1183e-02,\n",
      "        -2.4932e-03,  2.3515e-02,  1.9381e-02, -1.3205e-02,  3.1228e-02,\n",
      "        -3.2277e-03,  2.7640e-03, -2.7303e-02, -3.9087e-02,  3.1514e-02,\n",
      "         2.9302e-02,  1.1288e-02,  2.2016e-02,  7.1572e-02,  3.4191e-02,\n",
      "         6.3823e-02, -2.3841e-02, -2.3065e-02, -6.2991e-02,  1.1088e-03,\n",
      "        -6.7302e-03,  5.7249e-02, -1.7515e-02,  5.7028e-02,  2.2025e-02,\n",
      "         8.7595e-03,  4.9282e-02, -1.1063e-02, -4.4842e-02,  1.8474e-02,\n",
      "         4.9588e-02, -3.5636e-02,  6.1576e-04,  5.0259e-02, -1.3081e-02,\n",
      "        -5.1298e-02,  9.1870e-03, -6.9590e-02,  3.5853e-02,  3.8473e-02,\n",
      "        -2.8633e-02, -4.3386e-02, -4.6749e-02, -2.0138e-02, -6.2139e-02,\n",
      "         8.6133e-03,  9.1680e-03,  3.3844e-03, -5.5577e-02,  6.8060e-03,\n",
      "        -2.3076e-02,  4.1293e-02,  4.0208e-02,  5.9805e-02, -2.8993e-02,\n",
      "         3.7546e-02,  4.7754e-02,  1.8374e-02,  6.3102e-02, -1.7092e-02,\n",
      "         3.4811e-02, -8.0027e-03,  8.9707e-03, -2.6325e-02, -1.2745e-02,\n",
      "        -2.0420e-02,  1.9778e-02, -1.7719e-02, -7.3937e-03,  6.5132e-02,\n",
      "        -1.1574e-02,  5.8908e-03,  4.9681e-02,  7.8739e-02, -2.0202e-02,\n",
      "         2.5545e-02,  6.6226e-03,  7.8026e-02,  3.2636e-02, -7.5519e-02,\n",
      "        -5.1572e-03, -8.4546e-03,  3.4267e-03, -1.6402e-02,  9.7901e-03,\n",
      "         1.0011e-02, -5.2163e-03, -1.5828e-02, -9.8565e-02,  2.1989e-02,\n",
      "         1.6778e-02,  1.1089e-02,  3.7510e-02,  1.5064e-02,  4.9329e-02,\n",
      "        -4.5102e-02, -2.0284e-02,  3.3556e-02,  3.0209e-02, -1.1739e-02,\n",
      "         1.9679e-03, -7.6845e-04,  9.0960e-02, -7.1825e-03, -6.9168e-02,\n",
      "         5.7517e-02, -3.3272e-03,  7.4019e-03, -3.4702e-02,  4.6954e-02,\n",
      "         7.2332e-02, -7.8366e-03,  5.0512e-02, -2.4182e-02, -2.2244e-02,\n",
      "        -1.0271e-02, -5.9050e-02,  1.2271e-02, -1.4168e-02, -4.5758e-03,\n",
      "         3.1559e-02,  4.6442e-03,  5.7388e-03,  4.0388e-02,  6.6554e-02,\n",
      "         3.4883e-02, -2.4137e-02,  3.7357e-02, -1.8236e-02,  8.7921e-02,\n",
      "        -3.6749e-02,  1.7117e-02,  1.5561e-02,  2.0626e-03,  3.7509e-02,\n",
      "         3.9242e-02,  3.3025e-02, -2.1290e-02, -3.8387e-02, -9.0168e-02,\n",
      "        -4.7089e-02, -4.0643e-02, -8.2625e-02, -4.2897e-02, -7.4153e-03,\n",
      "        -4.1371e-02, -8.4637e-03,  3.4755e-02, -4.6047e-02,  3.9902e-02,\n",
      "         5.4072e-03, -4.5569e-02,  4.0827e-02, -3.0068e-04, -1.6392e-02,\n",
      "         5.7768e-03,  6.1603e-03, -2.2872e-02,  1.2226e-02, -1.8474e-02,\n",
      "         2.6411e-02, -2.0368e-02,  2.1834e-02,  1.3859e-03, -3.4495e-02,\n",
      "         1.3876e-02,  1.1448e-02,  3.3638e-02, -1.7301e-02, -1.5417e-02,\n",
      "         5.7110e-02, -9.4473e-03,  8.6540e-03, -4.5888e-02,  8.8702e-03,\n",
      "        -5.5239e-02, -2.4172e-02,  3.4171e-02, -5.6012e-03,  2.7971e-02,\n",
      "        -4.2354e-02, -1.7960e-02,  1.9870e-02, -3.0311e-02,  8.2919e-02,\n",
      "         1.8301e-03,  6.7294e-03,  4.3810e-02,  5.3055e-02, -2.5754e-02,\n",
      "         5.8432e-02, -6.0390e-02, -1.5664e-02,  8.8223e-02, -3.8425e-02,\n",
      "        -1.1664e-02,  1.9949e-02,  5.0815e-02, -2.1617e-02, -7.2865e-02,\n",
      "        -1.3952e-02, -8.6427e-03, -2.9556e-02,  7.4675e-02, -6.8725e-02,\n",
      "         3.3244e-02, -8.6201e-04,  5.4242e-03, -2.9845e-02,  4.4617e-02,\n",
      "         6.0957e-02, -8.5250e-03,  2.2715e-02,  4.6283e-02,  4.3347e-02,\n",
      "         1.6431e-02,  1.7999e-02, -3.3909e-02,  2.2619e-02,  4.7578e-02,\n",
      "        -2.8671e-02, -3.1803e-02,  2.2505e-02,  1.9369e-02,  2.5616e-02,\n",
      "        -4.3524e-02,  1.4006e-02, -4.5421e-02, -1.2097e-02, -1.4124e-02,\n",
      "        -3.4944e-02, -1.3290e-03,  4.7345e-02, -1.8227e-02, -1.3898e-02,\n",
      "        -1.2174e-02, -1.0356e-02, -4.1290e-02,  3.0488e-02, -4.5333e-02,\n",
      "        -5.0291e-02, -5.4177e-02, -3.5149e-02, -6.5772e-02,  1.9996e-02,\n",
      "         4.0644e-02, -6.4905e-02, -2.7647e-02,  3.7687e-02,  3.9374e-02,\n",
      "         1.3995e-02, -9.0119e-03, -5.2374e-02, -2.1023e-02,  1.7171e-02,\n",
      "         2.6499e-02,  5.7533e-02,  1.5248e-02,  3.8046e-02, -2.5348e-02,\n",
      "         1.8916e-02,  4.3500e-02, -7.1894e-02, -4.1639e-02, -1.5208e-02,\n",
      "         1.3708e-02, -4.8471e-02,  9.4243e-03,  3.8975e-02,  4.6978e-02,\n",
      "        -3.8600e-02,  1.9763e-02, -1.8312e-02,  2.3932e-02,  4.2268e-02,\n",
      "         4.1723e-02, -1.6330e-02, -2.0890e-03, -1.5566e-02,  3.1928e-02,\n",
      "         4.5370e-02,  4.3261e-02,  5.3644e-02, -4.0463e-02, -6.5456e-02,\n",
      "         1.7705e-02,  8.4796e-03, -7.7399e-02, -3.7641e-02, -2.5937e-02,\n",
      "        -4.3614e-02,  5.6867e-02, -3.0572e-02, -6.3149e-02, -1.1088e-02,\n",
      "         3.1678e-02, -5.5604e-03, -6.8970e-02,  2.0431e-02,  2.5088e-02,\n",
      "         7.2366e-02, -8.2228e-03,  2.2895e-02,  2.5867e-04, -1.2774e-02,\n",
      "        -5.8844e-03,  1.3492e-02, -2.3396e-02, -2.0113e-02,  9.9748e-03,\n",
      "         3.7351e-02, -2.2405e-03,  9.7515e-02, -5.6218e-02,  1.0498e-02,\n",
      "        -4.7524e-02,  6.4253e-03,  1.5660e-02,  4.5943e-02, -2.2011e-02,\n",
      "         2.9100e-02,  2.5660e-02,  3.5364e-03, -2.5880e-02,  3.5734e-03,\n",
      "         2.9188e-04, -1.0915e-02,  2.0526e-02,  3.4863e-02,  2.6400e-02,\n",
      "         1.5477e-02, -5.2770e-02, -5.7930e-02, -2.7159e-02,  5.7907e-02,\n",
      "         4.0208e-02,  1.2559e-02, -9.2701e-03, -7.2422e-02,  2.8152e-02,\n",
      "        -9.0453e-02,  1.7562e-02,  8.2023e-02, -3.5474e-02, -3.9066e-02,\n",
      "        -2.9743e-02,  2.4747e-02,  5.4680e-02, -1.8818e-03, -2.1271e-02,\n",
      "         4.5396e-02, -6.4806e-02,  5.5599e-03,  6.4097e-02,  1.6929e-02,\n",
      "         1.4922e-02, -6.3863e-03,  3.8912e-02, -6.4971e-03, -2.4414e-02,\n",
      "        -7.5869e-02,  7.2925e-02,  2.3312e-02,  5.7868e-02,  1.7230e-02,\n",
      "         5.5474e-02,  3.6404e-02, -1.1838e-02, -5.1777e-02, -3.6185e-02,\n",
      "         7.5587e-02, -3.8920e-03,  2.1252e-02,  2.2914e-03,  4.4097e-02,\n",
      "        -2.6922e-02, -7.0209e-02, -2.2841e-02, -1.2890e-03,  6.7283e-03,\n",
      "        -2.5325e-02,  2.9517e-02, -4.8269e-02,  1.5713e-02, -2.2317e-03,\n",
      "         2.4172e-03,  5.3563e-02, -1.2421e-02, -2.3192e-03,  3.7425e-02,\n",
      "        -2.1113e-02,  1.1337e-02,  5.0242e-02, -4.8910e-02, -3.3427e-02,\n",
      "         3.4319e-04,  2.1303e-03, -2.5791e-02, -8.8968e-03, -1.8605e-02,\n",
      "         2.2322e-02, -4.2391e-02, -9.2946e-03,  4.7215e-02, -2.9457e-02,\n",
      "         6.8644e-03, -5.2564e-02, -7.2020e-02,  3.8774e-02, -7.6307e-03,\n",
      "         1.4990e-02,  6.0824e-02, -7.2325e-02,  2.0455e-02,  9.1146e-03,\n",
      "         2.6738e-03, -5.5499e-03,  7.4548e-02,  4.0118e-02, -5.6896e-02,\n",
      "         5.1135e-02, -1.0977e-02,  3.3954e-03,  1.9806e-02,  3.3446e-04,\n",
      "         6.9535e-02,  2.6197e-02,  1.5176e-03,  4.0054e-02, -2.6855e-02,\n",
      "        -8.5468e-02,  4.7610e-02, -4.1366e-02,  2.4223e-02,  7.9580e-02,\n",
      "        -7.8680e-03, -1.3597e-02,  1.0123e-01, -4.1748e-02, -4.4384e-02,\n",
      "        -1.8421e-02, -1.7532e-02, -6.4129e-03,  2.4431e-02,  7.8045e-02,\n",
      "         4.2432e-02, -1.4813e-02, -4.5135e-02, -1.2087e-02,  6.0554e-02,\n",
      "        -1.5357e-02,  7.0638e-02, -8.2401e-02, -5.1496e-02, -2.5589e-02,\n",
      "        -7.1724e-03, -2.4414e-02, -2.9686e-02, -7.0918e-02,  6.0533e-02,\n",
      "        -3.3562e-03,  2.4738e-02,  5.3197e-02, -5.8904e-03,  7.4235e-02,\n",
      "        -4.5098e-02,  2.8189e-02, -3.9441e-02,  3.1614e-02,  1.2384e-02,\n",
      "        -3.0841e-02,  1.4992e-02,  1.4915e-02, -1.4999e-02, -4.3175e-02,\n",
      "        -3.5337e-02,  2.9950e-02,  2.4785e-02, -5.9327e-02,  4.2102e-03,\n",
      "         6.1831e-02, -7.0025e-03,  3.1780e-02, -4.9590e-02, -3.1339e-02,\n",
      "         2.6139e-02,  4.6000e-02,  6.0749e-03, -6.3536e-03,  6.3155e-03,\n",
      "         4.2068e-02, -6.4456e-02,  3.7959e-02,  6.8200e-02,  3.7568e-02,\n",
      "         7.1069e-03,  7.5376e-02, -1.4310e-02,  6.9223e-04, -2.8691e-02,\n",
      "        -5.7000e-03,  1.7889e-03, -1.0212e-01,  1.2737e-02,  6.5725e-02,\n",
      "         3.8445e-03,  2.8357e-02, -2.3658e-02,  2.7966e-02,  1.1758e-02,\n",
      "         2.1503e-02, -2.1867e-02, -5.3639e-02,  5.4962e-02, -9.1246e-03,\n",
      "        -2.0287e-03, -8.7026e-03, -1.5621e-02,  6.9507e-02,  1.0672e-02,\n",
      "         4.6337e-02, -9.2695e-03,  4.0680e-02,  2.7927e-02,  1.3263e-02,\n",
      "         4.4886e-02, -5.1781e-02, -1.8023e-02,  3.2122e-02, -2.6550e-02,\n",
      "         1.9788e-02,  4.1468e-02,  4.6687e-03, -2.7506e-02,  5.1200e-02,\n",
      "        -3.6105e-03, -1.5294e-02,  5.0176e-02, -1.4294e-02,  1.0602e-02,\n",
      "        -8.2720e-02,  4.3098e-02,  2.4990e-02, -2.7334e-02, -1.2119e-02,\n",
      "         1.4588e-02, -8.3672e-03, -6.7008e-02, -1.5341e-02, -3.6593e-02,\n",
      "         1.8821e-02,  6.5429e-02,  3.3050e-02,  3.4512e-02, -3.5626e-02,\n",
      "         1.4050e-02,  4.9985e-02, -3.5542e-03,  7.9198e-03,  2.7798e-02,\n",
      "         2.4950e-02,  1.7276e-02,  1.1369e-02,  2.1013e-02,  1.9493e-02,\n",
      "        -4.1396e-02,  3.3824e-02, -6.7837e-02,  2.5251e-02, -2.0896e-03,\n",
      "        -1.9399e-02, -9.4251e-02,  9.6831e-03, -3.9466e-02,  1.2230e-04,\n",
      "         2.5864e-02, -1.2253e-02,  3.3841e-02,  3.4191e-02,  6.5660e-02,\n",
      "        -3.0095e-02,  1.6516e-02, -4.0963e-02,  3.0938e-02,  2.6450e-02,\n",
      "        -2.6508e-03, -5.2572e-02, -5.8584e-02,  9.1168e-03,  4.7671e-02,\n",
      "         1.2817e-02,  3.3045e-03,  5.1033e-02,  4.3228e-03, -7.4014e-03,\n",
      "        -2.8152e-02, -4.8277e-02, -2.1375e-02,  5.9254e-02, -6.0067e-03,\n",
      "         2.9589e-02, -6.0591e-02,  5.5437e-02, -8.9927e-03, -7.8059e-03,\n",
      "        -3.6717e-03, -8.8069e-03, -2.8847e-02,  9.1289e-03,  2.8508e-02,\n",
      "         3.0514e-02,  3.9795e-03,  4.7787e-02,  3.2412e-02,  4.5845e-02,\n",
      "         4.0280e-02,  6.3165e-02, -8.8506e-04,  3.7910e-02,  1.0261e-02,\n",
      "         7.3254e-02,  5.1064e-02, -5.8960e-03,  3.0384e-02,  3.0182e-02,\n",
      "        -5.8728e-02, -1.4859e-02,  4.4516e-02, -6.5508e-02, -4.8366e-02,\n",
      "        -4.0869e-02, -1.0505e-02,  9.8312e-04, -6.7260e-02,  2.6057e-02,\n",
      "        -2.6408e-02,  2.2239e-02,  9.9732e-02,  3.6618e-02, -2.9901e-03,\n",
      "        -8.7594e-02, -2.9807e-02,  2.1636e-02,  5.4980e-02,  3.7167e-02,\n",
      "         3.1134e-02,  1.0440e-02,  6.7459e-03, -1.2164e-02,  8.6399e-03,\n",
      "        -1.5716e-02,  5.4732e-02, -4.4453e-02,  3.3579e-02,  7.6259e-04,\n",
      "        -3.4744e-02,  3.9834e-02,  1.3743e-02,  2.1529e-04,  8.0833e-02,\n",
      "        -3.6843e-02, -2.6403e-02,  1.3113e-02,  9.5883e-03,  4.7256e-02,\n",
      "        -5.9895e-02,  1.3163e-02,  8.8913e-02,  9.5657e-03,  5.8236e-02,\n",
      "         1.6446e-02,  1.9675e-03,  1.6855e-02, -1.2479e-03, -2.5898e-02,\n",
      "         6.8805e-02,  1.9395e-03, -4.3183e-02,  1.7171e-02,  1.0242e-02,\n",
      "        -4.3535e-02, -5.2188e-02,  4.8417e-02,  4.0333e-03, -4.1430e-03],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eteco\\AppData\\Local\\Temp\\ipykernel_11960\\1079021095.py:13: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  x = (x @ self.embedding.weight).T\n"
     ]
    }
   ],
   "source": [
    "# Test the whole thing\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "transformer = Transformer(num_blocks=2, config=config)\n",
    "x = torch.ones(config.d_vocab, dtype=torch.float)\n",
    "y: Float[torch.Tensor, \"vocab seq_len\"] = transformer(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f7f1",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cedf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss: torch.nn.MSELoss = nn.MSELoss()\n",
    "config: Config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "model: Transformer = Transformer(num_blocks=2, config=config)\n",
    "lr: Float = 1e-3\n",
    "optimizer: torch.optim.Adam = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
