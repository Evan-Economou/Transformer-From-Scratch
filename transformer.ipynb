{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "533bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../../../../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int\n",
    "    d_head: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e41103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_qk = nn.Linear(config.d_model, config.d_head)\n",
    "        self.W_vo = nn.Linear(config.d_model, config.d_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def create_mask(self, n_c: int) -> torch.Tensor:\n",
    "        mask = torch.triu(-1 * torch.inf * torch.ones(n_c, n_c), diagonal=0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        #create mask, with size n_c x n_c\n",
    "        mask = self.create_mask(x.shape[1])\n",
    "\n",
    "        #compute attention scores\n",
    "        A = self.softmax((self.W_qk(x) @ x.transpose() + mask)) @ self.W_vo(x)\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(num_blocks)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d392391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
