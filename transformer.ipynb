{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int\n",
    "import requests\n",
    "import unicodedata\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05314450",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../../../../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[list[str]]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee444a7",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, raw_data: str):\n",
    "        \"\"\"! @brief Initializes the Tokenizer with raw text data, processing and tokenizing it, and builds the vocabulary\n",
    "        @param raw_data The raw text data to be tokenized and used for building the vocabulary\"\"\"\n",
    "        self.raw_data = raw_data \n",
    "        tokenized_text = self.process_raw_data(raw_data).split(' ')\n",
    "\n",
    "        vocab_counts: Counter[str] = Counter(tokenized_text).most_common()\n",
    "\n",
    "        self.vocab_size: int = len(vocab_counts)\n",
    "\n",
    "        #vocab inverse takes integers and turns them back into words\n",
    "        self.vocab_inverse: list[str] = [token for token, _ in vocab_counts]\n",
    "\n",
    "        #vocab takes words and turns them into integers\n",
    "        self.vocab: dict[str: int] = dict(zip(self.vocab_inverse,range(0,self.vocab_size)))\n",
    "\n",
    "        print(self.vocab)\n",
    "        print(self.vocab_inverse)\n",
    "        \n",
    "    def process_raw_data(self, text: str, \n",
    "                        allowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "                        punctuation_convert: dict[str,str] = {'â€”': '-'}\n",
    "                        ) -> str:\n",
    "        \"\"\"! @brief Processes raw text data by removing punctuation, adding spaces, and removing excess whitespace\n",
    "        @param text The raw text data to be processed\n",
    "        @param allowed_punctuation A string of punctuation characters that should be preserved from the raw text\n",
    "        @param punctuation_convert A dictionary mapping punctuation characters to their replacements\n",
    "        @return The processed text with punctuation removed, spaces added, and excess whitespace removed \n",
    "        \"\"\"\n",
    "        for char, replacement in punctuation_convert.items():\n",
    "            text = text.replace(char, replacement)\n",
    "              \n",
    "        text = '\\n'.join(\n",
    "                    line \n",
    "                    for line in text.split('\\n')\n",
    "                    if '.jpg' not in line\n",
    "                )\n",
    "        \n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "        # Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        # remove newlines and tabs\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "        for char in allowed_punctuation:\n",
    "            text = text.replace(char, f' {char} ')\n",
    "              \n",
    "        text = text.strip()\n",
    "\n",
    "        # remove multiple spaces\n",
    "        while '  ' in text:\n",
    "            text = text.replace('  ', ' ')\n",
    "\n",
    "        text = ''.join((char if (char.isalnum() or char in allowed_punctuation or char == ' ') else ' ') for char in text)\n",
    "        \n",
    "        text = text.lower()\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    #I dont think we need this function\n",
    "    # def tokenize(self, \n",
    "    #     text: str,\n",
    "    #     process: bool = False,\n",
    "    # ) -> str:\n",
    "    #     if process:\n",
    "    #         text = self.process_raw_data(text)\n",
    "    #     tokenized_text = text.split(' ')\n",
    "\n",
    "    #     return tokenized_text\n",
    "    \n",
    "    def add_data(self, new_data: str):\n",
    "        \"\"\"! @brief Adds new raw text data to the Tokenizer, processing and tokenizing it, and updates the vocabulary\n",
    "        @param new_data The new raw text data to be added to the Tokenizer\"\"\"\n",
    "        self.raw_data += \" \" + new_data\n",
    "        tokenized_text = self.process_raw_data(self.raw_data).split(' ')\n",
    "\n",
    "        vocab_counts: Counter[str] = Counter(tokenized_text).most_common()\n",
    "\n",
    "        #update vocab size, vocab inverse, and vocab\n",
    "        self.vocab_size: int = len(vocab_counts)\n",
    "        self.vocab_inverse: list[str] = [token for token, _ in vocab_counts]\n",
    "        self.vocab: dict[str: int] = dict(zip(self.vocab_inverse,range(0,self.vocab_size)))\n",
    "\n",
    "    #should turn string of words into numbers\n",
    "    def encode(self, data: str) -> Int[torch.Tensor, \"n_context\"]:\n",
    "        data = self.process_raw_data(data).split(' ')\n",
    "        return torch.tensor([self.vocab[word] for word in data],dtype=torch.int)\n",
    "    \n",
    "    #should turn numbers into a string of words\n",
    "    def decode(self, tokens: Int[torch.Tensor, \"n_context\"]) -> str:\n",
    "        return \" \".join([self.vocab_inverse[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int\n",
    "    tokenizer: Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e41103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_qk = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_vo = nn.Linear(config.d_model, config.d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def create_mask(self, n_c: int) -> torch.Tensor:\n",
    "        mask: Float[torch.Tensor, \"n_context n_context\"] = torch.triu(-1 * torch.inf * torch.ones(n_c, n_c), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        #create mask, with size n_c x n_c\n",
    "        mask = self.create_mask(x.shape[0])\n",
    "\n",
    "        #compute attention scores\n",
    "        # A = softmax((X @ W_qk @ X^T) + M) @ X @ W_vo\n",
    "        A = self.softmax((self.W_qk(x)) @ x.transpose(0, -1) + mask) @ self.W_vo(x)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attention_head = AttentionHead(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "        return x + self.attention_head(x) + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Linear(config.d_vocab, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(num_blocks)])\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"vocab n_context\"]:\n",
    "        x_onehot = torch.zeros(x.shape[0], self.config.d_vocab)\n",
    "        for i, token in enumerate(x):\n",
    "            x_onehot[i, token] = 1.0\n",
    "        x = self.embedding(x_onehot)\n",
    "        # print(x.shape)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = (x @ self.embedding.weight)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "    def generate_output(self, x:str, n_tokens: int = 10) -> str:\n",
    "        output_str = \"\"\n",
    "        for _ in range(n_tokens):\n",
    "            last_logit = self.forward(self.config.tokenizer.encode(x))[-1,:]\n",
    "            idx = torch.argmax(last_logit)\n",
    "            x.append(self.config.tokenizer.decode(idx))\n",
    "            output_str.append(self.config.tokenizer.decode(idx))\n",
    "            \n",
    "        return output_str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0dc71",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8979a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aaaah': 0, 'im': 1, 'tokenizing': 2, 'it': 3}\n",
      "['aaaah', 'im', 'tokenizing', 'it']\n",
      "tensor([3, 1, 2], dtype=torch.int32)\n",
      "im im im\n",
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Attention head test\n",
    "x: Float[torch.Tensor, \"n_context d_model\"] = torch.ones(5, 16)\n",
    "tokenizer = Tokenizer(raw_data=\"Aaaah Im Tokenizing It\")\n",
    "\n",
    "print(tokenizer.encode(\"it im tokenizing\"))\n",
    "print(tokenizer.decode(torch.tensor([1,1,1],dtype=torch.int)))\n",
    "\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64,tokenizer=tokenizer)\n",
    "attention_head: AttentionHead = AttentionHead(config)\n",
    "output: Float[torch.Tensor, \"n_context d_model\"] = attention_head.forward(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d392391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000])\n",
      "tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 2, 1, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Test the whole thing\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64,tokenizer=tokenizer)\n",
    "transformer = Transformer(num_blocks=2, config=config)\n",
    "x = torch.tensor([0, 2, 1, 3], dtype=torch.int)\n",
    "y: Float[torch.Tensor, \"vocab n_context\"] = transformer(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f7f1",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cedf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: Transformer,\n",
    "    raw_data: str,\n",
    "    loss_fn: torch.nn.CrossEntropyLoss = nn.CrossEntropyLoss(),\n",
    "    lr: Float = 1e-3,\n",
    "    epochs: Int = 1\n",
    "    ):\n",
    "    optimizer: torch.optim.SGD = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    training_data = model.config.tokenizer.encode(raw_data)\n",
    "    true_values = torch.zeros(training_data.shape[0], model.config.d_vocab)\n",
    "    for i, token in enumerate(x):\n",
    "            true_values[i, token] = 1.0\n",
    "    n = training_data.length()\n",
    "    true_values = true_values[1:n,:]\n",
    "    for epoch in range(epochs):\n",
    "        outputs = model(training_data)\n",
    "        loss=loss_fn(outputs[0:n-1,:],true_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Loss for epoch {epoch}: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config: Config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "# model = Transformer(num_blocks=2, config=config)\n",
    "# train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
