{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05314450",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "533bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../../../../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[list[str]]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee444a7",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b63bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e41103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff83a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_qk = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_vo = nn.Linear(config.d_model, config.d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def create_mask(self, n_c: int) -> torch.Tensor:\n",
    "        mask: Float[torch.Tensor, \"seq_len seq_len\"] = torch.triu(-1 * torch.inf * torch.ones(n_c, n_c), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        #create mask, with size n_c x n_c\n",
    "        mask = self.create_mask(x.shape[0])\n",
    "\n",
    "        #compute attention scores\n",
    "        # A = softmax((X @ W_qk @ X^T) + M) @ X @ W_vo\n",
    "        A = self.softmax((self.W_qk(x)) @ x.transpose(0, -1) + mask) @ self.W_vo(x)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fd1570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attention_head = AttentionHead(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        return x + self.attention_head(x) + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "590a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Linear(config.d_vocab, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(num_blocks)])\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len vocab\"]) -> Float[torch.Tensor, \"vocab seq_len\"]:\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = (x @ self.embedding.weight).T\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0dc71",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a8979a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Attention head test\n",
    "x: Float[torch.Tensor, \"seq_len d_model\"] = torch.ones(5, 16)\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "attention_head: AttentionHead = AttentionHead(config)\n",
    "output: Float[torch.Tensor, \"seq_len d_model\"] = attention_head.forward(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d392391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor([ 1.3376e-02,  9.5112e-02,  1.6399e-02, -8.2028e-02, -8.6612e-03,\n",
      "        -1.9070e-02,  1.9305e-02, -6.0633e-03,  3.0350e-02,  1.6588e-02,\n",
      "         7.5139e-02,  6.9059e-03,  1.8125e-02,  1.2030e-02, -2.3526e-02,\n",
      "         3.0621e-02, -3.6794e-02,  7.1347e-02,  5.2588e-02,  1.6709e-03,\n",
      "         7.0914e-02,  2.3044e-02,  1.1746e-02,  2.3141e-02, -1.6804e-02,\n",
      "         1.3218e-03,  5.3172e-02, -5.3854e-02,  5.0984e-02, -2.0084e-02,\n",
      "        -5.3118e-02, -1.1259e-02, -2.6655e-02,  6.2946e-02,  2.0495e-02,\n",
      "        -3.1951e-02, -3.3592e-02,  5.1752e-02,  8.0289e-03,  5.1782e-02,\n",
      "         2.5465e-02,  8.5261e-03, -1.4020e-02,  9.6227e-04,  4.4673e-03,\n",
      "         2.7667e-02,  1.7083e-02,  4.4350e-02,  5.1626e-03,  3.6788e-02,\n",
      "         7.3341e-03,  1.4450e-02,  1.1233e-02, -1.2624e-02, -2.0046e-02,\n",
      "         3.4609e-02, -2.3734e-02,  4.8267e-02,  2.2080e-02, -7.2080e-03,\n",
      "         4.7682e-02,  5.9896e-02, -3.1297e-02,  1.5619e-02, -2.6683e-02,\n",
      "         2.4478e-02, -3.7535e-02,  8.1732e-02, -2.4422e-02, -2.3748e-02,\n",
      "         3.6160e-02, -4.6263e-02, -5.0925e-02,  9.2194e-03,  2.6735e-02,\n",
      "         2.8691e-02, -2.7623e-02,  1.7926e-02,  5.8296e-02, -1.6470e-02,\n",
      "         9.7508e-03,  4.6498e-02,  9.6798e-02,  7.4773e-02,  6.4069e-02,\n",
      "        -3.3738e-02,  5.9456e-03,  5.1560e-02,  5.0390e-02,  3.6298e-02,\n",
      "         2.9491e-02,  4.0985e-03,  2.7107e-02, -2.7816e-02, -2.9057e-02,\n",
      "        -2.8034e-02, -1.5281e-03,  3.9547e-02, -1.0147e-02, -2.3672e-02,\n",
      "         7.3839e-03,  2.9452e-03,  3.0934e-02, -2.2212e-02,  6.5568e-02,\n",
      "        -5.4545e-03,  4.8846e-02, -4.3755e-03, -5.7372e-03, -3.1647e-02,\n",
      "         7.3105e-02, -3.9224e-02, -1.0170e-02, -4.6001e-02,  2.6830e-02,\n",
      "        -2.3402e-02, -2.1639e-02,  1.2948e-02,  3.4896e-02, -4.1790e-02,\n",
      "        -2.6463e-03,  3.3675e-02,  3.9697e-02,  1.4824e-03, -1.6710e-02,\n",
      "        -2.5012e-02, -3.9610e-02, -4.8634e-02,  7.3964e-02, -2.2815e-02,\n",
      "        -1.6529e-02,  1.7067e-02, -2.8182e-02,  1.4040e-02,  5.7889e-02,\n",
      "         5.2097e-02,  4.0190e-02,  1.1658e-02,  4.3179e-03, -3.5079e-02,\n",
      "        -1.3640e-02, -3.0017e-02,  3.8627e-02, -6.3058e-03, -2.5701e-02,\n",
      "         1.7724e-02,  1.8327e-02, -6.6491e-03, -4.1213e-02, -4.7689e-02,\n",
      "         3.5421e-02, -2.4611e-02, -6.4315e-03,  8.5713e-02, -2.2185e-03,\n",
      "        -4.4162e-04, -2.6921e-02,  2.2819e-02, -3.7286e-02, -1.0447e-02,\n",
      "         2.6987e-02, -1.3437e-02,  1.6440e-02, -7.4007e-03, -1.3717e-02,\n",
      "        -3.3299e-02,  4.0456e-02,  1.7756e-02,  6.7548e-02, -6.4025e-02,\n",
      "         3.8517e-02,  1.3550e-02, -3.9806e-02, -2.0282e-02,  2.9669e-02,\n",
      "         3.7048e-02, -9.0530e-03,  1.7751e-02,  2.9600e-03, -6.1091e-02,\n",
      "        -5.7474e-02,  2.1261e-02, -1.1695e-02,  6.6206e-04, -3.4783e-02,\n",
      "        -2.5231e-02,  4.7329e-02,  7.7137e-03, -9.7090e-04, -9.2780e-02,\n",
      "         3.7573e-02, -4.9457e-02, -8.7033e-04, -2.6513e-02, -2.7762e-02,\n",
      "        -5.6255e-03,  1.9100e-02,  7.2903e-03,  3.8044e-02,  1.8431e-02,\n",
      "         3.8878e-03, -5.5121e-02, -4.8614e-02, -2.2127e-03,  6.1863e-03,\n",
      "        -2.8547e-02, -2.9842e-02, -1.2414e-02,  6.1771e-02,  3.0138e-02,\n",
      "         2.6345e-02, -1.4562e-02,  7.8799e-02,  2.0639e-02,  2.5056e-04,\n",
      "        -1.6001e-02,  7.4607e-03, -1.1566e-02, -3.4841e-02,  2.4504e-02,\n",
      "         1.8852e-02, -8.5136e-03, -1.6060e-02, -1.7576e-02,  3.7249e-02,\n",
      "        -1.9794e-02,  4.4395e-02,  7.4619e-02,  4.5023e-03, -1.9836e-02,\n",
      "        -1.4993e-02,  1.7429e-02, -5.5093e-02,  3.0424e-02,  3.0840e-02,\n",
      "        -4.9044e-02,  2.7069e-02,  6.7068e-03, -1.4082e-02, -3.2969e-02,\n",
      "         4.3253e-03,  6.2885e-02, -3.3027e-02,  1.8410e-02,  6.1170e-02,\n",
      "         2.7915e-03,  4.6172e-02, -1.3094e-02, -1.5244e-02,  1.4814e-02,\n",
      "        -5.8538e-02, -1.4570e-02,  2.1108e-02,  2.7068e-03,  4.4968e-02,\n",
      "        -1.9595e-02,  8.9654e-03,  7.8497e-02,  2.0255e-02,  2.5200e-03,\n",
      "         2.0101e-02,  6.4419e-03, -6.9097e-02, -7.7062e-03,  2.7999e-02,\n",
      "        -1.3955e-02, -4.3777e-02, -1.5713e-02,  5.2086e-02,  2.8334e-02,\n",
      "         5.0488e-02, -4.1180e-02, -5.8094e-02,  2.3248e-02,  8.0187e-03,\n",
      "        -1.3384e-02,  5.5277e-03,  1.3760e-02,  7.1004e-02,  5.0016e-02,\n",
      "         3.9230e-02,  4.0883e-02, -2.4476e-02, -1.0162e-02, -3.1181e-02,\n",
      "         6.8178e-03,  8.5661e-05,  9.2295e-03, -4.2729e-03, -1.6885e-02,\n",
      "         1.8553e-02, -2.3880e-03,  4.4643e-02,  4.9125e-03, -1.6569e-02,\n",
      "        -2.2630e-02,  4.0192e-02, -3.2599e-02, -3.5714e-03,  2.4332e-02,\n",
      "         2.5095e-02, -2.6293e-02, -3.8910e-02, -1.3336e-02, -5.4554e-02,\n",
      "        -3.1847e-02,  8.6659e-02, -2.3238e-02,  3.0623e-02, -1.3672e-02,\n",
      "        -6.1579e-02, -2.3007e-02,  7.3952e-02, -1.1261e-02,  1.6460e-02,\n",
      "        -6.0158e-03, -1.0701e-02,  5.6191e-02,  8.6905e-03, -1.3705e-02,\n",
      "        -1.0407e-02,  2.4166e-02,  6.4464e-03, -1.0790e-02,  4.7064e-02,\n",
      "        -4.4874e-02, -2.1104e-02, -3.4205e-03,  1.4032e-02,  5.7745e-02,\n",
      "        -1.1601e-02,  2.5359e-02,  4.0320e-04, -1.6821e-02,  4.1061e-02,\n",
      "         1.5685e-02, -1.8948e-02,  1.0410e-02,  4.8517e-02, -8.7174e-04,\n",
      "         5.7144e-03,  1.6254e-02,  7.7593e-03,  4.0948e-02,  5.2291e-03,\n",
      "        -1.2451e-02,  5.1437e-03,  1.7508e-02, -1.7154e-02, -6.9356e-02,\n",
      "        -3.2902e-02,  5.3964e-03, -1.5318e-02,  3.4690e-02,  1.5664e-02,\n",
      "         9.1286e-03, -1.4634e-02,  5.5235e-02,  5.5838e-03, -9.9732e-03,\n",
      "         3.6565e-02,  1.7303e-02, -5.8877e-02,  4.7372e-03, -2.5231e-02,\n",
      "        -6.9074e-02,  6.3471e-02, -7.0749e-02,  8.9393e-02,  9.1812e-03,\n",
      "         5.6227e-03, -6.4805e-02, -3.0380e-02,  3.9128e-02, -6.5968e-02,\n",
      "         2.7333e-02,  1.0381e-02, -6.9829e-02, -3.8018e-02,  1.5459e-02,\n",
      "         1.9914e-02,  4.7757e-02, -1.2866e-02, -5.3388e-02, -9.5975e-03,\n",
      "         4.0039e-02, -3.1099e-02,  1.2053e-02, -2.3600e-02,  2.2362e-02,\n",
      "        -5.3602e-03,  4.0082e-03,  4.4702e-03,  3.2334e-02, -5.4411e-03,\n",
      "         6.2449e-02,  3.3643e-02, -5.0253e-03,  9.9657e-03,  5.6391e-02,\n",
      "         5.4211e-02,  1.1571e-02,  7.9590e-02,  4.1936e-04,  1.1451e-02,\n",
      "        -6.0665e-03, -2.0934e-02, -2.9701e-02, -8.2984e-03, -9.2242e-03,\n",
      "        -2.9345e-03,  4.3313e-03, -7.3213e-03,  3.7954e-03,  2.2642e-02,\n",
      "         1.2933e-02,  4.1608e-02,  6.8868e-03, -1.7429e-02,  8.8335e-03,\n",
      "        -1.5984e-02, -2.7200e-02, -2.0610e-02, -3.5981e-02,  1.9823e-02,\n",
      "         5.0270e-02,  8.0840e-03, -3.1728e-02, -8.7188e-02,  4.3006e-02,\n",
      "         6.9502e-03,  1.6906e-02,  3.8412e-02, -2.4404e-02,  8.2946e-02,\n",
      "        -2.6221e-02, -5.7706e-02,  7.0773e-02,  2.3307e-02,  6.9415e-03,\n",
      "         2.5136e-02,  2.4953e-03, -1.5189e-02,  4.4642e-02,  1.3721e-03,\n",
      "         2.6939e-02,  6.4795e-03,  1.1507e-02, -1.8013e-02,  4.6640e-02,\n",
      "        -1.9876e-02, -1.0128e-02, -1.0579e-02,  7.7280e-02,  3.6116e-03,\n",
      "         1.2156e-02,  3.0427e-02,  2.9735e-02, -2.8683e-02, -4.0267e-02,\n",
      "        -2.8196e-02,  1.9054e-02, -1.6912e-03,  6.4997e-03,  8.6517e-03,\n",
      "        -2.9257e-03, -5.6106e-03,  4.4310e-02,  2.9004e-02,  1.3363e-02,\n",
      "         3.5800e-02,  1.3064e-02, -1.1454e-02,  2.8785e-02, -3.5112e-02,\n",
      "        -3.4226e-02, -4.9036e-02,  4.1889e-02, -7.1510e-02,  1.7415e-02,\n",
      "         1.9659e-02,  5.1655e-02, -1.4933e-02,  5.9760e-04,  1.0087e-02,\n",
      "         2.7414e-02,  8.0301e-02, -3.2084e-02, -2.5367e-02,  3.1909e-02,\n",
      "        -2.5231e-02, -9.0538e-03,  1.2600e-02,  1.9276e-02, -5.3678e-03,\n",
      "         3.8393e-02,  7.8066e-02, -8.2680e-02, -7.3426e-03, -7.3605e-02,\n",
      "        -2.9371e-03, -3.6740e-03,  3.5949e-03, -6.0621e-03, -1.6665e-02,\n",
      "         3.5165e-02,  3.4781e-02,  2.5161e-02,  4.2916e-02, -6.8872e-03,\n",
      "         1.6728e-02, -4.4091e-03, -2.1077e-02, -3.0210e-02, -9.2025e-03,\n",
      "         3.4125e-02, -9.2754e-03, -3.8229e-02, -5.0126e-03, -2.5437e-02,\n",
      "         2.2174e-02,  7.3853e-02,  3.6098e-02, -1.3331e-02,  6.5587e-03,\n",
      "         5.0830e-04, -3.3857e-02,  6.4790e-02, -7.7710e-02,  6.0886e-02,\n",
      "        -6.6040e-02, -1.0708e-01, -4.9918e-02,  1.5418e-02, -4.1502e-02,\n",
      "        -1.2390e-02, -2.2767e-02,  2.8607e-02,  5.2173e-02,  3.9517e-02,\n",
      "         2.4657e-02,  3.6306e-02,  8.7706e-03, -1.8905e-03,  4.8201e-03,\n",
      "         2.1398e-02, -1.8537e-02, -4.6834e-02, -1.4792e-02,  1.3108e-02,\n",
      "         4.3606e-02, -1.2929e-02, -1.4376e-02,  1.7246e-02,  3.2860e-02,\n",
      "         3.3748e-02,  1.9521e-02, -2.1531e-03, -7.9171e-02,  2.8341e-02,\n",
      "        -1.6813e-02, -4.0405e-02, -3.8560e-02, -7.2657e-02,  1.0106e-02,\n",
      "         2.6250e-03,  8.6184e-03, -7.6591e-02, -5.8186e-03, -1.6321e-02,\n",
      "        -7.6288e-02, -2.3613e-02, -2.1853e-02,  1.2262e-02,  2.0121e-02,\n",
      "         1.1821e-02, -7.1751e-03, -3.0580e-02,  2.5414e-03, -3.8479e-02,\n",
      "        -1.1844e-02,  1.1648e-02,  1.1576e-02,  5.8502e-03,  8.7680e-03,\n",
      "         1.9827e-02,  1.4191e-02, -3.1915e-02, -4.7268e-03, -6.7280e-03,\n",
      "        -2.7204e-02, -9.7065e-02, -5.8522e-02, -2.5375e-02,  1.4310e-02,\n",
      "        -3.2622e-02,  6.5786e-02, -3.1918e-03,  2.9276e-02,  2.1997e-02,\n",
      "         1.3891e-03,  4.5965e-02, -3.0447e-02, -5.7665e-04, -8.5496e-03,\n",
      "        -2.1886e-02,  3.6888e-02,  7.3002e-02,  3.0108e-02, -1.9927e-02,\n",
      "        -5.7119e-02, -7.6447e-03,  1.3703e-03, -1.3211e-02, -1.2131e-03,\n",
      "        -2.7429e-02,  8.4071e-03,  4.0235e-02,  5.8290e-02, -2.1552e-02,\n",
      "         1.6918e-02, -4.4963e-02,  1.2768e-02,  6.5219e-02, -1.4054e-02,\n",
      "        -5.1689e-03, -4.6095e-02, -5.8335e-02, -6.1764e-03, -2.5670e-02,\n",
      "         1.3340e-02, -6.2522e-03,  5.2114e-03,  1.5446e-02,  1.7199e-02,\n",
      "        -5.7093e-03, -2.1894e-02,  5.3721e-02, -1.6376e-02,  4.9811e-02,\n",
      "        -1.9303e-02,  5.7231e-03, -5.0859e-02, -2.2650e-02,  1.9488e-02,\n",
      "        -3.9498e-02,  2.0515e-02, -1.9009e-02, -2.2420e-03,  2.4870e-02,\n",
      "         3.6176e-02, -6.1091e-03, -5.0427e-02,  9.0900e-02,  1.2597e-02,\n",
      "         9.7217e-03,  2.4342e-02,  7.7372e-02,  1.0663e-02,  4.6013e-02,\n",
      "         6.5240e-02, -1.8659e-02, -5.2192e-02, -2.3880e-02,  1.4877e-02,\n",
      "         1.0829e-02, -2.2966e-02, -7.3582e-05,  1.7757e-03,  1.7725e-02,\n",
      "        -6.9694e-02,  1.3792e-02,  8.5264e-03,  2.6958e-02,  6.3677e-02,\n",
      "        -3.6183e-02, -2.5010e-02,  1.9030e-03, -6.2548e-04, -1.0956e-02,\n",
      "        -1.1278e-02, -5.8895e-02, -2.6801e-02, -4.2233e-02, -3.5321e-02,\n",
      "         1.4992e-02,  4.0325e-02,  3.3924e-02, -1.7421e-02,  1.4390e-02,\n",
      "         6.6082e-02,  3.3230e-02,  5.7807e-03, -5.0158e-02,  3.1891e-02,\n",
      "         1.9421e-02, -2.6202e-02, -1.2174e-02, -6.2968e-02, -1.1831e-02,\n",
      "         1.6538e-02,  9.0393e-03,  5.6748e-02,  4.4989e-02, -1.3977e-02,\n",
      "        -8.1679e-03,  2.3865e-02,  2.7857e-02, -3.5763e-02,  2.5227e-02,\n",
      "        -6.2885e-02, -2.1804e-02,  1.1796e-02, -4.5958e-02,  1.7864e-02,\n",
      "         2.6676e-02, -4.2851e-02, -1.2050e-04,  1.6339e-02,  1.5934e-02,\n",
      "         9.4858e-04,  1.1414e-02,  1.5067e-02, -1.6837e-02,  3.2638e-02,\n",
      "        -2.5113e-02,  2.3416e-02,  1.7988e-02,  1.8841e-02,  9.5276e-03,\n",
      "        -2.5228e-02, -2.3341e-02, -4.7983e-02,  1.3555e-02, -2.1056e-02,\n",
      "        -3.3858e-02,  7.8549e-02,  2.9491e-02,  4.1940e-02,  3.5398e-02,\n",
      "        -1.4845e-02, -6.5479e-02, -3.8476e-02,  3.7672e-02,  4.3117e-02,\n",
      "         7.6364e-02,  1.4367e-02, -1.4283e-02,  4.0326e-03, -9.1882e-04,\n",
      "         1.9411e-02, -9.7448e-03,  4.4192e-02, -3.8631e-02,  2.0512e-02,\n",
      "        -3.3034e-02,  3.0098e-02,  6.0953e-02, -6.2854e-02,  1.2700e-02,\n",
      "         7.2136e-02, -4.5677e-02, -1.0350e-02,  6.5615e-02,  6.7813e-02,\n",
      "        -9.5954e-02,  4.6268e-02,  9.5313e-03,  1.2974e-02, -6.4232e-04,\n",
      "         5.3410e-02,  5.9684e-02, -3.1882e-02, -5.6899e-02, -2.8185e-02,\n",
      "        -8.5355e-03, -4.4195e-02, -1.6892e-02,  2.1895e-02, -4.5860e-02,\n",
      "         5.4201e-02,  1.1521e-02, -3.8466e-02, -4.7715e-02, -5.1258e-02,\n",
      "         6.7278e-02,  5.3052e-02, -3.0806e-02, -4.6852e-03, -5.0119e-02,\n",
      "        -3.7734e-04,  7.1698e-03, -2.4843e-02, -1.7949e-02,  4.6886e-02,\n",
      "         1.7216e-02,  5.3461e-02, -5.2056e-03,  5.4562e-02, -1.3992e-03,\n",
      "         5.9522e-02,  4.6287e-02, -2.9741e-02,  2.2139e-02, -1.7498e-02,\n",
      "        -5.2448e-02, -1.2331e-06, -1.7661e-02, -7.0908e-02,  2.4204e-02,\n",
      "         1.6858e-02,  6.2521e-02, -1.5878e-02, -8.3896e-04,  4.8801e-02,\n",
      "         3.2879e-02,  1.2790e-03, -3.3178e-02, -7.6307e-03,  4.7560e-02,\n",
      "         1.4048e-02, -1.7154e-02, -2.5879e-02, -6.5332e-03,  8.2036e-03,\n",
      "        -2.7412e-02,  3.0677e-02, -1.2408e-03, -2.3266e-02,  1.3846e-02,\n",
      "         1.6989e-02, -3.2659e-03, -3.7411e-02, -1.6765e-02,  1.8828e-02,\n",
      "        -2.2609e-04,  3.2840e-02,  3.0591e-02, -1.9360e-02, -1.4658e-02,\n",
      "         2.7755e-02,  5.7122e-02,  2.0153e-02, -2.0055e-02,  1.5911e-03,\n",
      "        -1.3787e-02, -5.2954e-02,  3.5407e-02,  3.0098e-03, -3.5390e-02,\n",
      "        -5.9979e-02, -3.9248e-02,  4.9964e-02, -2.4390e-02,  1.1667e-03,\n",
      "         2.6235e-02,  1.1506e-02, -9.6467e-03, -8.0099e-03, -2.2734e-02,\n",
      "         5.3236e-02,  1.3836e-02, -4.8520e-02, -2.6777e-02, -8.0570e-02,\n",
      "         6.6583e-04,  1.3789e-02,  3.0052e-02, -6.0766e-03, -1.9535e-02,\n",
      "        -4.5049e-02,  2.6045e-02, -1.0320e-01,  5.9557e-03,  3.7344e-02,\n",
      "        -3.9648e-03, -4.0069e-02, -4.4391e-02,  1.5206e-02, -2.6608e-02,\n",
      "         3.1345e-02,  6.3460e-02,  6.9963e-02, -1.7156e-02, -8.9900e-03,\n",
      "        -5.9712e-02, -3.6771e-02,  4.6329e-02,  7.6137e-02, -3.0931e-02,\n",
      "        -1.1744e-02,  3.2992e-02,  2.2084e-03,  1.3807e-02, -4.7080e-03,\n",
      "         1.0264e-02, -6.7899e-03, -3.0681e-02, -2.8880e-02,  4.6420e-02,\n",
      "         3.9157e-02,  4.3165e-02, -1.3396e-02, -6.0541e-03,  3.6342e-02,\n",
      "        -3.4507e-02,  2.3675e-02,  4.3113e-02,  6.6245e-02,  2.8819e-02,\n",
      "         3.5163e-02,  3.4650e-02, -1.9692e-02, -3.0368e-02,  2.2013e-02,\n",
      "         3.4440e-02, -6.6100e-02, -3.4949e-02,  5.4257e-02, -3.7344e-02,\n",
      "         6.8156e-02,  5.7142e-02, -6.8661e-02, -4.1034e-03,  4.8248e-02,\n",
      "        -3.4485e-02,  2.5815e-04,  2.2110e-02,  1.3577e-02, -4.6920e-02,\n",
      "         1.5818e-02,  2.1644e-02,  2.5426e-03,  3.3813e-02, -5.0811e-02,\n",
      "        -4.7681e-02, -4.7682e-02, -1.2943e-02,  2.4181e-02,  3.6742e-02,\n",
      "         2.2705e-02, -1.9245e-02, -1.3939e-02, -5.3608e-03,  4.2602e-02,\n",
      "        -4.9228e-03,  5.7255e-03, -4.5336e-02, -1.8428e-02,  8.0058e-04,\n",
      "        -2.0801e-02, -7.4913e-03, -1.3133e-02, -1.5720e-02, -1.2348e-02,\n",
      "         1.5586e-03,  9.6111e-04,  1.9938e-02,  1.7106e-02,  6.0681e-02,\n",
      "         5.8281e-03,  1.1277e-01, -4.7336e-02, -2.0741e-02, -2.9822e-02,\n",
      "        -5.1721e-03,  1.1990e-02,  3.1133e-02, -2.2961e-02,  1.0314e-01,\n",
      "         1.5140e-02,  2.6271e-03,  3.4015e-02, -2.1602e-02, -3.4486e-02,\n",
      "         1.6431e-02,  3.2476e-02, -6.8705e-02, -2.1260e-02, -2.5904e-03,\n",
      "         1.3505e-02,  4.7349e-02, -3.5750e-02,  3.0386e-02,  2.2913e-02,\n",
      "         3.5465e-02,  8.7566e-03, -3.0863e-02,  1.1069e-01,  3.4595e-02,\n",
      "        -2.8142e-03, -2.6026e-02,  1.8621e-02, -2.3953e-02, -1.2148e-02,\n",
      "         1.2774e-02, -1.6026e-02, -2.6870e-02, -1.2651e-02, -4.8840e-02],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Test the whole thing\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "transformer = Transformer(num_blocks=2, config=config)\n",
    "x = torch.ones(config.d_vocab, dtype=torch.float)\n",
    "y: Float[torch.Tensor, \"vocab seq_len\"] = transformer(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f7f1",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85cedf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: Transformer,\n",
    "    loss: torch.nn.CrossEntropyLoss = nn.CrossEntropyLoss(),\n",
    "    lr: Float = 1e-3,\n",
    "    epochs: Int = 1\n",
    "    ):\n",
    "    optimizer: torch.optim.SGD = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    string_data = get_many_books(ids=range(10,15), data_temp=\"./data/gutenberg_data\")\n",
    "    labels = {}\n",
    "    int_labels = []\n",
    "    for i in range(len(string_data)):\n",
    "        labels[i] = string_data[i]\n",
    "        int_labels.append(i)\n",
    "    \n",
    "    training_data = torch.tensor(int_labels)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(training_data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89cd1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 10...\n",
      "\t4432261 characters read\n",
      "Getting book 11...\n",
      "\t148062 characters read\n",
      "Getting book 12...\n",
      "\t168390 characters read\n",
      "Getting book 13...\n",
      "\t34579 characters read\n",
      "Getting book 14...\n",
      "\t1951150 characters read\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config: Config \u001b[38;5;241m=\u001b[39m Config(d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, d_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, d_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(num_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss, lr, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab seq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m     12\u001b[0m         x \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "config: Config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "model = Transformer(num_blocks=2, config=config)\n",
    "train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
