{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int\n",
    "import requests\n",
    "import unicodedata\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05314450",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../../../../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[list[str]]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee444a7",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e41103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff83a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_qk = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_vo = nn.Linear(config.d_model, config.d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def create_mask(self, n_c: int) -> torch.Tensor:\n",
    "        mask: Float[torch.Tensor, \"seq_len seq_len\"] = torch.triu(-1 * torch.inf * torch.ones(n_c, n_c), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        #create mask, with size n_c x n_c\n",
    "        mask = self.create_mask(x.shape[0])\n",
    "\n",
    "        #compute attention scores\n",
    "        # A = softmax((X @ W_qk @ X^T) + M) @ X @ W_vo\n",
    "        A = self.softmax((self.W_qk(x)) @ x.transpose(0, -1) + mask) @ self.W_vo(x)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd1570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attention_head = AttentionHead(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        return x + self.attention_head(x) + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Linear(config.d_vocab, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(num_blocks)])\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len vocab\"]) -> Float[torch.Tensor, \"vocab seq_len\"]:\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = (x @ self.embedding.weight).T\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, raw_data: str):\n",
    "        self.raw_data = raw_data \n",
    "        self.vocab_size = None\n",
    "        self.vocab = None\n",
    "        self.vocab_inverse = None\n",
    "        \n",
    "    def process_raw_data(self, text: str, \n",
    "                        allowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "                        punctuation_convert: dict[str,str] = {'â€”': '-'}\n",
    "                        ) -> str:\n",
    "        for char, replacement in punctuation_convert.items():\n",
    "            text = text.replace(char, replacement)\n",
    "              \n",
    "        text = '\\n'.join(\n",
    "                    line \n",
    "                    for line in text.split('\\n')\n",
    "                    if '.jpg' not in line\n",
    "                )\n",
    "        \n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "        # Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        # remove newlines and tabs\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "        for char in allowed_punctuation:\n",
    "            text = text.replace(char, f' {char} ')\n",
    "              \n",
    "        text = text.strip()\n",
    "\n",
    "        # remove multiple spaces\n",
    "        while '  ' in text:\n",
    "            text = text.replace('  ', ' ')\n",
    "\n",
    "        text = ''.join((char if (char.isalnum() or char in allowed_punctuation or char == ' ') else ' ') for char in text)\n",
    "        \n",
    "        text = text.lower()\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, \n",
    "        text: str,\n",
    "        process: bool = False,\n",
    "    ):\n",
    "        if process:\n",
    "            text = self.process_raw_data(text)\n",
    "        tokenized_text = text.split(' ')\n",
    "\n",
    "        vocab_counts: Counter[str] = Counter(tokenized_text).most_common()\n",
    "        self.vocab_size: int = len(vocab_counts)\n",
    "        self.vocab: list[str] = [token for token, count in vocab_counts]\n",
    "        self.vocab_inverse: dict[int: str] = {key: value for key, value in (self.vocab, range(0, self.vocab_size))}\n",
    "        \n",
    "    def encode(self, data: str) -> list[int]:\n",
    "        return [self.vocab_inverse[word] for word in data]\n",
    "    \n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return ''.join(self.vocab[token] for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0dc71",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8979a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Attention head test\n",
    "x: Float[torch.Tensor, \"seq_len d_model\"] = torch.ones(5, 16)\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "attention_head: AttentionHead = AttentionHead(config)\n",
    "output: Float[torch.Tensor, \"seq_len d_model\"] = attention_head.forward(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d392391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor([-5.5714e-02,  1.8208e-02,  2.2584e-03, -4.0544e-02,  2.2233e-03,\n",
      "        -6.9042e-03,  2.7877e-02,  3.5115e-02, -1.4944e-02,  5.9888e-03,\n",
      "        -6.9910e-02,  1.9059e-02,  7.3378e-03,  3.6847e-05, -1.4642e-02,\n",
      "         5.9656e-02,  3.7088e-02,  2.3467e-03, -3.6977e-03, -1.9625e-02,\n",
      "         1.7052e-02, -9.2979e-03, -6.0489e-02, -2.3581e-02, -3.8072e-02,\n",
      "        -1.9473e-03,  5.2006e-02,  1.1954e-02, -7.5672e-02,  2.3997e-02,\n",
      "        -2.6352e-02,  2.0462e-03,  3.5036e-02,  1.8057e-03,  5.6882e-02,\n",
      "         7.0652e-02,  1.5618e-02, -9.6903e-03, -4.2127e-04,  4.1136e-02,\n",
      "         7.8641e-03, -2.6780e-03,  3.3360e-02,  3.0056e-02, -1.5967e-02,\n",
      "        -3.3927e-02, -8.1995e-02, -6.5845e-02,  1.6618e-02,  1.2403e-02,\n",
      "        -1.3927e-02, -5.3685e-02, -2.7333e-02, -5.1440e-02, -7.7847e-02,\n",
      "         1.8380e-02, -1.6425e-02,  2.2449e-02, -1.8473e-02, -3.5723e-02,\n",
      "        -5.1367e-02, -2.6521e-02,  1.4641e-02, -2.1195e-02, -1.5389e-02,\n",
      "         1.1004e-02, -1.6729e-02,  5.6949e-02,  1.6304e-02,  2.2454e-02,\n",
      "        -3.0745e-02,  2.7098e-02,  3.3492e-02,  4.9862e-02, -2.8681e-03,\n",
      "        -3.5116e-02,  3.2218e-02,  4.6648e-02,  3.4750e-02,  3.1582e-02,\n",
      "         1.1188e-02,  2.1453e-02,  1.9675e-02, -3.7701e-03,  2.9320e-02,\n",
      "         4.1253e-02, -2.3997e-02,  1.6046e-02,  8.3396e-03,  2.2959e-02,\n",
      "        -3.8798e-03, -1.7066e-03,  4.0669e-03,  7.7967e-03,  4.2795e-02,\n",
      "        -5.3751e-02, -5.1700e-02, -1.1789e-02,  2.3528e-02,  6.0089e-04,\n",
      "        -2.4670e-02, -2.6356e-02, -8.0679e-03,  8.9404e-02,  9.5062e-03,\n",
      "        -9.9382e-02,  1.9088e-02,  4.4249e-02,  3.6197e-03, -3.4521e-03,\n",
      "        -2.0694e-02, -1.5534e-02, -6.5187e-03,  3.6287e-02,  5.7764e-03,\n",
      "         4.6973e-02, -7.4121e-02,  1.4850e-02, -3.9001e-02,  2.4979e-02,\n",
      "        -1.7762e-02,  1.4041e-02,  3.9052e-02,  8.2613e-03, -7.9617e-03,\n",
      "         5.4460e-02, -3.1504e-02,  4.7765e-02, -6.2924e-02, -3.1597e-02,\n",
      "         3.3783e-02,  1.0930e-02, -5.9993e-02,  2.6110e-03,  4.7903e-02,\n",
      "         2.7416e-02, -1.6805e-02,  3.9441e-02, -7.5969e-03, -2.0779e-03,\n",
      "         3.5907e-02,  1.7428e-02, -2.0797e-02, -3.6088e-02, -2.9019e-02,\n",
      "         2.9331e-02,  8.0450e-02,  4.1890e-02,  1.4302e-02,  6.5192e-03,\n",
      "        -3.2613e-02,  1.2510e-02,  5.1549e-02, -8.8188e-03, -1.2939e-02,\n",
      "         5.3731e-02, -1.5686e-02,  2.5703e-02,  6.1263e-03,  1.2844e-03,\n",
      "        -2.2380e-02,  5.1242e-03,  2.8020e-02,  2.5196e-02,  5.0730e-03,\n",
      "         4.6943e-02,  8.3389e-04, -1.6765e-02,  7.3959e-03, -7.8579e-03,\n",
      "        -2.1917e-02, -4.9261e-02, -4.5586e-02, -1.5355e-02,  3.8809e-02,\n",
      "         2.6857e-02, -2.8518e-02,  6.9302e-02,  3.1996e-02,  3.2045e-02,\n",
      "        -2.5036e-02, -1.5728e-02, -8.9102e-03, -3.3268e-04,  6.2941e-02,\n",
      "         4.4653e-02,  4.4823e-02,  8.7773e-03, -4.0137e-02,  6.6985e-02,\n",
      "        -4.7816e-02, -3.5588e-02, -4.5267e-03, -1.5337e-02, -4.6657e-02,\n",
      "        -3.5943e-02,  3.1540e-02,  4.1580e-02, -3.5514e-02,  2.9773e-02,\n",
      "        -3.4456e-02,  3.3904e-02,  2.4748e-02,  6.1437e-02,  2.1159e-02,\n",
      "         5.6845e-02, -2.7289e-02, -1.1222e-02,  2.0127e-02,  9.8648e-03,\n",
      "        -3.6432e-02, -3.1590e-02, -3.5638e-03,  7.1065e-02,  2.2894e-02,\n",
      "        -1.0364e-02, -2.2452e-02,  1.7250e-02, -4.0256e-02,  2.2415e-02,\n",
      "         1.5987e-02, -3.6701e-02, -5.8748e-02,  3.2205e-03,  2.4910e-02,\n",
      "         5.5874e-02, -2.5853e-02, -2.6176e-02,  7.2541e-03,  3.0466e-02,\n",
      "         7.4707e-02,  1.0275e-02, -2.8278e-02, -8.4399e-03, -6.3806e-02,\n",
      "         2.2162e-02,  2.1061e-02,  2.9498e-03, -1.4498e-02, -3.9856e-02,\n",
      "         3.7647e-03,  6.5612e-02,  2.2899e-02, -6.3440e-03, -1.0609e-02,\n",
      "         1.6382e-02, -2.8198e-02,  4.6242e-03, -3.1914e-02,  7.9690e-03,\n",
      "        -1.2517e-02, -6.1001e-02,  1.1609e-02,  6.3104e-02,  2.0655e-02,\n",
      "        -1.7527e-02,  2.3127e-02,  2.4997e-02,  1.2664e-02, -2.0361e-02,\n",
      "        -6.1000e-02,  1.0007e-01, -2.7273e-02,  1.2936e-02,  2.2348e-02,\n",
      "         5.1492e-02,  8.1012e-03, -1.4489e-02,  2.9957e-03, -2.0016e-02,\n",
      "         2.0224e-04,  2.5841e-02,  5.1283e-02,  5.9332e-04,  6.3798e-02,\n",
      "        -9.1010e-03, -9.3443e-04, -5.7705e-02, -3.2860e-02, -2.5039e-02,\n",
      "        -2.4287e-02,  1.9672e-03,  1.5447e-02,  3.2644e-02,  3.8146e-03,\n",
      "        -2.4391e-02,  7.8519e-04, -3.0589e-02, -5.4293e-03, -1.5946e-03,\n",
      "         3.4407e-02,  1.7109e-02,  4.6492e-06, -3.2997e-02,  8.8678e-02,\n",
      "        -7.4182e-03, -1.8256e-02,  5.4856e-03,  4.9937e-02, -3.4130e-03,\n",
      "        -1.3541e-02, -1.9355e-02, -7.1000e-03, -1.1063e-05,  1.9249e-02,\n",
      "        -2.4539e-02, -5.0828e-02, -3.0504e-02, -6.6234e-02,  7.4376e-03,\n",
      "         4.9290e-03,  4.8022e-02,  1.0410e-02, -2.8870e-02,  3.2914e-02,\n",
      "        -6.1626e-03,  2.3757e-02, -2.2982e-02,  3.2574e-02, -3.7419e-03,\n",
      "         5.0597e-04,  7.1369e-02,  6.9313e-02,  4.6956e-02,  4.3885e-03,\n",
      "        -3.9212e-02,  3.2577e-02, -1.9173e-02, -9.0066e-03,  3.6559e-02,\n",
      "         1.5544e-02, -3.2611e-02,  2.7321e-02,  1.5695e-02, -1.7543e-02,\n",
      "         5.0349e-02, -3.1782e-03,  3.0265e-02,  4.4007e-02,  1.0389e-02,\n",
      "        -5.5822e-02, -3.8803e-03, -1.0186e-02,  1.1811e-02, -1.5532e-02,\n",
      "        -4.0578e-02,  5.1125e-02,  2.7923e-02,  8.6825e-03, -1.6729e-03,\n",
      "        -4.4970e-03,  1.7778e-02,  7.3305e-03,  6.3537e-03, -7.6300e-03,\n",
      "        -3.5736e-02, -2.3166e-03,  3.1633e-02,  3.7969e-02,  6.1651e-02,\n",
      "        -2.1735e-02,  6.2201e-03, -2.5949e-03,  2.5804e-03,  1.3571e-02,\n",
      "         1.7533e-02,  1.9718e-02, -3.3316e-03,  5.6857e-03, -3.1861e-02,\n",
      "         2.0071e-02,  3.4066e-02,  4.8593e-02, -3.8086e-02, -3.1906e-02,\n",
      "        -1.8841e-02, -2.5651e-02, -2.6407e-02,  1.4814e-02, -2.7240e-02,\n",
      "        -6.0985e-02,  6.2761e-02, -9.8417e-03, -1.5308e-02,  1.2762e-02,\n",
      "        -2.8441e-02, -5.2037e-04, -1.0645e-02, -5.1145e-02,  1.5509e-02,\n",
      "         5.2664e-02,  2.5115e-02,  9.3162e-03, -3.7635e-02,  1.7213e-02,\n",
      "         2.4327e-02, -2.0271e-02, -8.1551e-03,  6.0188e-02,  9.2701e-03,\n",
      "         2.2470e-02, -4.1559e-02, -8.9618e-02, -1.0370e-02,  1.7467e-02,\n",
      "         2.6909e-02,  5.0160e-03,  1.8914e-02, -1.8148e-02,  3.7863e-03,\n",
      "         5.0888e-03,  4.1811e-02, -2.8213e-02,  1.0514e-02, -3.9471e-02,\n",
      "         5.2630e-02, -1.4301e-02, -2.3096e-02, -2.7349e-02,  2.3422e-02,\n",
      "        -1.8841e-03, -3.3842e-03, -3.0165e-02,  9.3609e-03,  1.0379e-03,\n",
      "         9.1667e-03, -1.9088e-02,  6.0122e-02,  8.5806e-02, -3.0522e-02,\n",
      "        -2.9111e-02,  9.5125e-03,  5.8744e-02,  8.6934e-02,  2.8958e-02,\n",
      "        -3.0428e-02, -4.2178e-02, -9.3309e-03,  2.8466e-02, -2.5519e-02,\n",
      "        -5.2360e-02,  2.7183e-02,  8.5770e-03, -1.5221e-02,  2.3142e-03,\n",
      "         2.0167e-02,  4.2281e-02, -4.6567e-03, -2.7466e-03,  3.4144e-02,\n",
      "        -5.3451e-03,  7.5914e-02, -8.9321e-03, -3.4425e-02, -1.8831e-03,\n",
      "         4.1902e-03, -3.5567e-02, -7.4763e-03,  1.5877e-02,  8.7999e-03,\n",
      "         4.2627e-02,  3.8511e-02,  1.6317e-02,  5.3217e-02, -5.5385e-02,\n",
      "        -3.5423e-05,  2.6990e-02,  6.9734e-03,  3.3861e-02, -1.0391e-02,\n",
      "         7.1478e-03, -4.2039e-02, -5.1241e-02, -1.9339e-02,  3.7136e-02,\n",
      "        -1.8229e-02,  2.7906e-02, -3.8791e-04,  4.8725e-02, -1.9541e-02,\n",
      "         5.0786e-02,  1.2901e-02, -1.7196e-02,  1.3735e-02, -2.0953e-02,\n",
      "         2.6436e-02, -3.3830e-02, -2.2389e-02, -3.3331e-02, -2.1157e-02,\n",
      "         4.8996e-02,  2.3618e-02,  7.5580e-02, -4.8852e-02, -1.0585e-02,\n",
      "         2.0349e-02, -6.4249e-02, -1.7025e-02,  2.7617e-02, -2.2989e-02,\n",
      "        -3.2401e-02, -2.7535e-02,  2.6041e-03, -5.1801e-02, -7.1570e-03,\n",
      "        -5.0552e-03, -2.1298e-02,  2.6488e-02, -1.5123e-02,  4.7148e-02,\n",
      "        -6.3883e-02, -2.4296e-02,  2.3737e-02,  2.6706e-02,  2.3693e-02,\n",
      "         2.9572e-03, -1.0077e-02,  6.3667e-03,  3.0605e-02, -3.8239e-02,\n",
      "         1.1448e-02,  3.7656e-03,  1.2578e-03,  4.1370e-03, -2.0012e-02,\n",
      "         2.4353e-02,  4.7257e-02,  9.4381e-02, -1.0265e-02, -7.0333e-02,\n",
      "        -6.8515e-03, -1.8552e-02, -2.1449e-02, -5.0224e-02,  2.8773e-02,\n",
      "        -5.1384e-02, -3.7767e-03, -1.3575e-02,  2.1935e-02,  3.0091e-02,\n",
      "        -3.7045e-02,  2.0237e-02,  2.3824e-02,  1.8371e-02,  1.1908e-02,\n",
      "         4.1289e-02,  6.3960e-03,  3.3753e-02,  2.4645e-02,  4.4060e-03,\n",
      "        -3.0461e-02,  2.2691e-02,  6.6207e-02,  1.0922e-02,  2.7734e-02,\n",
      "        -1.7432e-02, -2.1951e-02,  2.9592e-03, -1.8662e-02, -2.7344e-02,\n",
      "         1.6963e-02, -4.9322e-02,  3.0211e-03,  4.6642e-02, -1.3715e-02,\n",
      "         7.7246e-02, -3.8635e-02,  1.1720e-03,  3.4591e-02, -2.2687e-02,\n",
      "        -2.9036e-02, -4.1645e-03,  1.7577e-02,  7.1157e-03,  3.8206e-02,\n",
      "        -3.0967e-02,  1.1356e-02,  2.0309e-02, -3.7473e-02, -2.4212e-02,\n",
      "        -9.0169e-03, -1.9408e-02,  4.4305e-02,  1.3938e-02, -1.0745e-02,\n",
      "        -9.1158e-03, -1.0328e-02,  1.7535e-02,  4.1048e-02, -1.7540e-03,\n",
      "         1.6101e-02, -5.0851e-04,  2.7606e-02,  2.6751e-02,  5.6453e-02,\n",
      "         4.4600e-02,  9.1602e-03,  1.6409e-02, -3.7127e-02, -8.2471e-03,\n",
      "        -1.8191e-03,  7.9300e-02,  1.6356e-02, -4.1969e-02, -2.1735e-02,\n",
      "        -4.2999e-02, -1.9388e-02,  4.9541e-02,  6.0256e-02,  2.8232e-02,\n",
      "         1.9261e-02,  1.2265e-02,  7.0069e-04, -3.0403e-02,  8.2819e-02,\n",
      "        -1.3100e-02,  1.0049e-02, -7.6838e-03, -2.8795e-02, -1.2537e-02,\n",
      "        -2.6124e-02, -1.9826e-02,  2.4059e-02,  5.8468e-02, -1.5578e-02,\n",
      "        -5.3393e-02, -4.0811e-02, -1.1089e-03,  4.7914e-03,  3.0686e-02,\n",
      "         1.1748e-02, -1.7782e-02,  3.3637e-02,  1.7353e-02,  1.2073e-03,\n",
      "        -1.9971e-03,  7.5335e-03, -3.2882e-02, -4.4885e-02, -6.0833e-02,\n",
      "         5.9749e-02,  2.1046e-03,  4.2045e-02, -2.4748e-02, -2.0253e-02,\n",
      "         1.8921e-03,  3.6437e-02,  2.3313e-02, -2.4970e-02,  4.8764e-02,\n",
      "         7.1908e-03, -5.8581e-03,  4.9587e-03,  3.2276e-02, -3.9574e-02,\n",
      "        -2.5170e-02,  6.6396e-02,  5.1486e-03, -1.2997e-02,  1.6998e-02,\n",
      "        -1.5556e-02, -8.1286e-03, -2.8074e-02, -2.0035e-03, -5.3257e-02,\n",
      "        -2.4346e-02,  2.6252e-02,  3.9960e-02, -2.7420e-02,  5.1423e-02,\n",
      "         1.5781e-02, -5.7958e-03, -8.4229e-03, -2.1823e-02, -1.3487e-02,\n",
      "         3.3088e-02, -3.2231e-02, -4.8599e-02, -8.7436e-03,  1.2006e-02,\n",
      "         4.1826e-02, -4.0585e-02,  3.4639e-02, -6.3050e-02,  5.3511e-02,\n",
      "        -9.3781e-03,  2.9011e-02,  1.0326e-02, -1.2075e-02, -1.0696e-02,\n",
      "         3.2756e-04, -7.6577e-02,  1.4606e-02, -1.5561e-02, -1.2705e-02,\n",
      "         1.8753e-02, -2.2507e-02,  1.4757e-02,  2.6995e-02, -1.5092e-02,\n",
      "         6.5403e-02, -3.1738e-02,  1.9735e-02, -8.6778e-02,  5.0609e-02,\n",
      "        -9.5518e-03, -6.4215e-02,  3.2234e-02, -3.8879e-02,  1.0245e-02,\n",
      "         3.8861e-02,  4.1669e-02, -2.1141e-02, -8.2195e-04,  6.9808e-02,\n",
      "         6.1752e-03,  3.0668e-03,  1.6730e-02, -4.3936e-02, -5.6742e-03,\n",
      "        -7.8498e-03, -1.4216e-02,  2.9794e-02,  3.9227e-03, -2.5440e-02,\n",
      "        -3.7937e-02,  1.9365e-02,  4.1069e-02,  3.5731e-03,  2.3337e-02,\n",
      "         1.5007e-02, -3.1267e-02,  3.8619e-02, -1.2730e-02,  1.3789e-03,\n",
      "         3.3612e-02, -1.5428e-02,  1.0116e-02,  8.1931e-02,  2.5128e-02,\n",
      "         2.8864e-02, -2.6464e-02, -3.7882e-02,  1.5852e-02,  2.3653e-02,\n",
      "        -2.1454e-02,  1.5214e-04, -7.6145e-02, -1.6020e-02, -1.1684e-02,\n",
      "        -3.2810e-02,  5.5072e-03, -9.2775e-03,  8.3224e-03, -2.8540e-02,\n",
      "        -1.2246e-03,  6.2987e-03,  1.8077e-02, -8.7525e-03, -8.2660e-03,\n",
      "         4.7360e-02, -3.7269e-02,  7.5947e-03,  5.7600e-03, -2.6707e-02,\n",
      "        -9.4203e-03,  5.6431e-04,  1.2963e-02,  2.2536e-02,  1.0690e-02,\n",
      "         4.9824e-03,  2.7383e-02, -2.1748e-02, -9.4095e-03, -5.2946e-02,\n",
      "        -7.7735e-02, -5.7952e-02, -1.1993e-02, -4.0127e-02,  3.6045e-02,\n",
      "        -1.3029e-02,  5.3396e-02,  2.8337e-02,  2.5074e-02, -2.6033e-02,\n",
      "         7.8432e-02, -5.7270e-03,  1.6737e-02, -8.8206e-04,  3.3753e-02,\n",
      "        -2.6160e-02, -2.5842e-02,  2.3555e-02,  7.3370e-02, -1.0577e-02,\n",
      "         7.1027e-03,  5.6646e-02,  5.2290e-02, -3.0696e-02,  9.3750e-03,\n",
      "         2.0772e-02,  3.5706e-02,  8.3670e-03, -7.1677e-03,  4.2218e-02,\n",
      "        -4.4593e-02,  9.1688e-03,  4.7244e-02,  3.1462e-02,  2.5110e-02,\n",
      "         6.3521e-02,  6.8231e-02,  3.5345e-02,  3.3182e-02,  5.0312e-03,\n",
      "        -4.4724e-02,  2.1656e-02,  3.7374e-02,  1.5344e-02,  9.2660e-03,\n",
      "         1.5173e-02, -5.0435e-02,  1.9427e-02, -9.8877e-03, -9.7534e-03,\n",
      "        -3.1639e-02, -9.6815e-03,  1.7260e-02,  9.5548e-03,  2.3422e-02,\n",
      "        -8.7425e-02,  3.1762e-02,  1.2021e-02, -8.2389e-03, -1.8137e-02,\n",
      "         4.1390e-02, -2.4223e-02,  2.3276e-02,  2.4708e-02, -1.6360e-03,\n",
      "         1.0500e-02, -3.4545e-02,  9.7485e-03, -5.3259e-02,  5.6892e-04,\n",
      "         1.9287e-02, -1.7519e-03,  2.7408e-03, -8.8532e-03, -3.2040e-02,\n",
      "        -2.4539e-02, -2.4036e-02, -4.4512e-03,  2.7757e-02,  6.8125e-03,\n",
      "        -3.9353e-02,  5.0230e-03, -1.6477e-03,  1.5641e-02,  2.7786e-02,\n",
      "         2.4688e-02,  1.4655e-02, -2.6991e-02,  3.5140e-02,  3.2790e-02,\n",
      "        -4.6682e-04,  8.3734e-03,  2.4042e-02, -4.6397e-02, -5.7786e-03,\n",
      "         2.2168e-02, -4.1123e-03, -3.0163e-02, -2.1387e-02,  3.6393e-03,\n",
      "         9.5434e-03, -7.2485e-03, -2.5447e-02, -3.1363e-02, -5.7469e-02,\n",
      "         1.1110e-02,  4.7822e-03, -3.0481e-02,  8.1544e-03, -1.0879e-02,\n",
      "         4.5238e-02, -7.0520e-04, -1.6501e-02, -3.8627e-02, -2.9560e-02,\n",
      "        -2.1366e-02, -6.3007e-02, -1.8544e-02, -1.1534e-02,  6.9130e-02,\n",
      "         3.4020e-02,  1.8102e-02,  1.1237e-03, -1.8122e-02,  6.0105e-03,\n",
      "        -1.4860e-02, -1.0847e-02, -3.2404e-02,  1.4527e-02, -6.7404e-03,\n",
      "        -3.8848e-02,  3.7379e-02, -1.7770e-02,  3.2631e-02, -6.8898e-03,\n",
      "        -2.6053e-02,  5.1860e-02,  2.0807e-02,  4.3597e-03, -1.1922e-02,\n",
      "         3.1248e-02, -3.8173e-03,  6.5795e-03,  1.1931e-02, -1.2280e-02,\n",
      "         3.0925e-02, -8.0416e-03,  6.2045e-02,  5.6668e-02, -4.3223e-02,\n",
      "         2.4320e-02,  2.9906e-02,  3.4763e-04,  1.7445e-02, -9.2826e-03,\n",
      "         3.0826e-03,  2.7203e-02,  1.5606e-02,  7.0491e-02, -2.4938e-02,\n",
      "         2.1075e-02, -1.9206e-02,  1.4868e-02, -2.8721e-02, -3.4454e-02,\n",
      "        -1.1846e-02, -2.9514e-02,  1.2795e-02,  3.4504e-02, -3.6137e-02,\n",
      "         2.2056e-02, -2.3323e-02,  9.0143e-03,  1.7063e-02,  6.9193e-03,\n",
      "         1.6097e-03,  1.5563e-03, -4.2979e-02,  3.4146e-02, -1.6140e-02,\n",
      "         4.1056e-02, -1.3119e-02, -1.9030e-02,  4.7681e-03, -1.1279e-03,\n",
      "         8.5845e-03, -1.9255e-02,  5.8020e-03,  5.4512e-02,  1.8525e-03,\n",
      "        -4.8552e-02,  2.2470e-02,  2.9133e-02, -1.6767e-02, -2.5069e-02,\n",
      "         4.4226e-02,  7.8429e-02, -4.3493e-02, -1.2341e-02, -3.9748e-02,\n",
      "         1.4225e-03,  2.2685e-03,  2.5673e-02,  4.0365e-02, -2.6783e-02,\n",
      "        -1.9843e-02, -1.1516e-02,  8.2175e-02, -2.4841e-02, -1.6045e-02,\n",
      "        -2.4310e-03, -1.1441e-02, -9.5688e-03,  3.9835e-02, -4.2783e-03,\n",
      "         1.7632e-02,  1.9334e-02, -4.7075e-03,  8.4105e-03, -3.2943e-02,\n",
      "         2.6013e-02,  4.2504e-02,  2.1234e-02,  4.3723e-02, -2.5360e-02],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan\\AppData\\Local\\Temp\\ipykernel_136360\\1079021095.py:13: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4419.)\n",
      "  x = (x @ self.embedding.weight).T\n"
     ]
    }
   ],
   "source": [
    "# Test the whole thing\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "transformer = Transformer(num_blocks=2, config=config)\n",
    "x = torch.ones(config.d_vocab, dtype=torch.float)\n",
    "y: Float[torch.Tensor, \"vocab seq_len\"] = transformer(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f7f1",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85cedf36",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (341824798.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    tokenized_string_data = [x for i in string_data[]]\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_model(\n",
    "    model: Transformer,\n",
    "    loss: torch.nn.CrossEntropyLoss = nn.CrossEntropyLoss(),\n",
    "    lr: Float = 1e-3,\n",
    "    epochs: Int = 1\n",
    "    ):\n",
    "    optimizer: torch.optim.SGD = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    string_data = get_many_books(ids=range(10,15), data_temp=\"./data/gutenberg_data\")\n",
    "    tokenized_string_data = [x for i in string_data[]]\n",
    "    labels = {}\n",
    "    int_labels = []\n",
    "    for i in range(len(string_data)):\n",
    "        labels[i] = string_data[i]\n",
    "        int_labels.append(i)\n",
    "    \n",
    "    training_data = torch.tensor(int_labels)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(training_data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 10...\n",
      "\t4432261 characters read\n",
      "Getting book 11...\n",
      "\t148062 characters read\n",
      "Getting book 12...\n",
      "\t168390 characters read\n",
      "Getting book 13...\n",
      "\t34579 characters read\n",
      "Getting book 14...\n",
      "\t1951150 characters read\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config: Config \u001b[38;5;241m=\u001b[39m Config(d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, d_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, d_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(num_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss, lr, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab seq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m     12\u001b[0m         x \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "config: Config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "model = Transformer(num_blocks=2, config=config)\n",
    "train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
