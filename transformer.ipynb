{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int\n",
    "import requests\n",
    "import unicodedata\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05314450",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../../../../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[list[str]]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee444a7",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63bede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e41103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear2 = nn.Linear(config.d_hidden, config.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff83a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.W_qk = nn.Linear(config.d_model, config.d_model)\n",
    "        self.W_vo = nn.Linear(config.d_model, config.d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def create_mask(self, n_c: int) -> torch.Tensor:\n",
    "        mask: Float[torch.Tensor, \"seq_len seq_len\"] = torch.triu(-1 * torch.inf * torch.ones(n_c, n_c), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        #create mask, with size n_c x n_c\n",
    "        mask = self.create_mask(x.shape[0])\n",
    "\n",
    "        #compute attention scores\n",
    "        # A = softmax((X @ W_qk @ X^T) + M) @ X @ W_vo\n",
    "        A = self.softmax((self.W_qk(x)) @ x.transpose(0, -1) + mask) @ self.W_vo(x)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd1570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attention_head = AttentionHead(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len d_model\"]) -> Float[torch.Tensor, \"seq_len d_model\"]:\n",
    "        return x + self.attention_head(x) + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590a6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Linear(config.d_vocab, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(num_blocks)])\n",
    "        \n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"seq_len vocab\"]) -> Float[torch.Tensor, \"vocab seq_len\"]:\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = (x @ self.embedding.weight).T\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, raw_data: str):\n",
    "        self.raw_data = raw_data \n",
    "        self.vocab_size = None\n",
    "        self.vocab = None\n",
    "        self.vocab_inverse = None\n",
    "        \n",
    "    def process_raw_data(self, text: str, \n",
    "                        allowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "                        punctuation_convert: dict[str,str] = {'â€”': '-'}\n",
    "                        ) -> str:\n",
    "        for char, replacement in punctuation_convert.items():\n",
    "            text = text.replace(char, replacement)\n",
    "              \n",
    "        text = '\\n'.join(\n",
    "                    line \n",
    "                    for line in text.split('\\n')\n",
    "                    if '.jpg' not in line\n",
    "                )\n",
    "        \n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "        # Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        # remove newlines and tabs\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "        for char in allowed_punctuation:\n",
    "            text = text.replace(char, f' {char} ')\n",
    "              \n",
    "        text = text.strip()\n",
    "\n",
    "        # remove multiple spaces\n",
    "        while '  ' in text:\n",
    "            text = text.replace('  ', ' ')\n",
    "\n",
    "        text = ''.join((char if (char.isalnum() or char in allowed_punctuation or char == ' ') else ' ') for char in text)\n",
    "        \n",
    "        text = text.lower()\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, \n",
    "        text: str,\n",
    "        process: bool = False,\n",
    "    ) -> list[int]:\n",
    "        if process:\n",
    "            text = self.process_raw_data(text)\n",
    "        tokenized_text = text.split(' ')\n",
    "\n",
    "        vocab_counts: Counter[str] = Counter(tokenized_text).most_common()\n",
    "        self.vocab_size: int = len(vocab_counts)\n",
    "        self.vocab = [token for token, count in vocab_counts]\n",
    "        self.vocab_inverse = {key: value for key, value in (range(0, self.vocab_size), self.vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0dc71",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8979a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Attention head test\n",
    "x: Float[torch.Tensor, \"seq_len d_model\"] = torch.ones(5, 16)\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "attention_head: AttentionHead = AttentionHead(config)\n",
    "output: Float[torch.Tensor, \"seq_len d_model\"] = attention_head.forward(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d392391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor([-3.0392e-02,  2.3679e-02,  4.2455e-03,  5.7964e-02,  1.8441e-02,\n",
      "         2.1536e-02,  4.0499e-03,  1.9642e-02,  4.8857e-02, -6.3464e-02,\n",
      "         6.5552e-03,  6.5082e-03, -1.8573e-02,  3.5810e-02,  4.8736e-02,\n",
      "        -7.7834e-02,  5.0728e-02,  3.0764e-02, -2.5055e-03,  1.9068e-02,\n",
      "        -3.6373e-03,  1.0339e-02,  1.5741e-02,  4.3782e-02, -2.3166e-02,\n",
      "        -3.5456e-02,  8.8922e-03,  2.3663e-02, -1.5116e-02, -4.0177e-03,\n",
      "        -2.5768e-02,  3.4168e-02, -1.0990e-02,  3.1342e-02, -9.2257e-03,\n",
      "         2.9073e-02, -3.3552e-02, -1.0607e-04,  4.8377e-03,  4.9110e-02,\n",
      "         1.7104e-03, -3.2923e-02, -1.0801e-02,  7.0406e-03, -7.8230e-03,\n",
      "         1.0830e-02,  2.8227e-02, -2.5068e-03,  6.6414e-03,  2.8692e-02,\n",
      "         8.0468e-03, -3.7961e-04, -8.3236e-03, -3.1042e-02, -4.7181e-02,\n",
      "        -7.2676e-04, -1.8590e-02,  1.3625e-02, -3.6877e-02,  3.2727e-02,\n",
      "         4.2861e-02, -3.6292e-02,  6.1446e-03,  3.9727e-02,  1.9036e-02,\n",
      "        -5.3235e-02,  5.4263e-02,  1.0383e-02, -2.0162e-02,  6.6242e-02,\n",
      "         4.9762e-03, -3.8244e-03, -1.0924e-02,  4.1153e-02, -4.1291e-02,\n",
      "         2.7106e-02,  5.2256e-02, -6.2853e-03,  2.9792e-03,  2.5555e-03,\n",
      "         4.2693e-02, -3.5898e-02, -2.1208e-02,  3.5085e-02,  6.2902e-02,\n",
      "         1.3563e-02, -3.7233e-02,  1.5934e-02, -4.5544e-02, -2.4949e-02,\n",
      "         1.5564e-02, -7.8406e-03,  3.3184e-02,  2.4735e-02, -2.4133e-02,\n",
      "        -1.5737e-02,  3.0377e-02,  1.3373e-02,  5.7155e-02,  2.1956e-02,\n",
      "         1.7323e-02, -3.9081e-02,  8.9169e-03,  1.3990e-02, -1.6288e-02,\n",
      "         2.6640e-02, -4.6321e-02, -2.0417e-02, -2.1599e-02, -5.2016e-03,\n",
      "        -3.8978e-03, -1.9348e-02,  3.5017e-02,  2.5025e-03,  4.8922e-02,\n",
      "         2.9101e-03, -3.8414e-03, -1.8623e-04, -1.1988e-02, -7.6815e-03,\n",
      "         4.5767e-03,  2.3785e-02, -3.8708e-03, -1.6112e-02,  3.2444e-02,\n",
      "         3.8461e-02, -3.8378e-02, -2.3313e-02,  5.1151e-03, -1.1494e-02,\n",
      "         4.2446e-02,  1.4428e-02,  3.3316e-02,  5.8627e-03,  3.1379e-02,\n",
      "         3.5351e-02, -4.4470e-02, -7.3300e-03,  8.9723e-03, -2.6282e-05,\n",
      "         2.8716e-03,  2.7879e-02, -7.9057e-04, -1.8541e-03, -3.9302e-03,\n",
      "        -2.6865e-02,  1.1001e-02,  3.9108e-02, -1.7145e-02, -4.8847e-02,\n",
      "        -1.5316e-03, -3.6360e-02, -3.5150e-02, -1.8558e-02, -4.3963e-05,\n",
      "         2.6872e-02, -5.8552e-02, -2.9460e-02, -1.2101e-03,  1.4580e-02,\n",
      "         1.2570e-02, -4.8238e-02,  3.8080e-02, -3.8359e-03, -8.3789e-03,\n",
      "         2.8228e-02, -3.6617e-02, -1.2174e-02,  6.1179e-02, -5.3998e-02,\n",
      "        -2.1181e-02,  7.5861e-02,  6.2851e-02, -2.5808e-02, -5.4204e-02,\n",
      "         1.4738e-03, -1.5005e-02, -7.1500e-04, -2.2817e-02,  2.3684e-02,\n",
      "        -4.5715e-02,  2.4661e-03,  3.1400e-02, -3.3656e-02,  2.5233e-02,\n",
      "        -3.5239e-02, -4.5555e-02, -2.4668e-02,  4.8288e-02,  3.7004e-02,\n",
      "         1.5608e-02, -2.2954e-02, -4.3582e-02, -3.1906e-02,  3.8130e-02,\n",
      "        -3.0048e-02,  8.3337e-03,  1.0703e-02, -2.1313e-03,  5.7866e-03,\n",
      "        -1.7675e-02,  1.6732e-02,  2.1045e-02, -2.3947e-02,  2.4869e-03,\n",
      "         1.9593e-02,  6.2548e-02,  6.6748e-03,  1.8028e-02,  7.5634e-03,\n",
      "        -2.9624e-02, -7.0368e-02, -3.1267e-02,  2.3607e-03,  3.3634e-02,\n",
      "         4.5835e-02,  6.9237e-02, -3.3477e-03, -6.9682e-03, -4.6722e-02,\n",
      "         4.0353e-02,  2.4649e-02, -7.7599e-02, -2.2722e-02,  3.1881e-02,\n",
      "         3.0431e-02,  1.6015e-02, -3.6243e-02,  1.2009e-05,  1.7362e-02,\n",
      "        -2.5105e-02,  5.6686e-03, -3.1282e-02, -5.3554e-02, -8.7471e-03,\n",
      "        -5.6274e-02, -1.0619e-02,  1.8118e-02,  7.6956e-03,  2.2896e-02,\n",
      "        -9.9889e-03,  1.8490e-02,  1.0030e-02,  2.1463e-02, -1.3028e-02,\n",
      "        -3.1783e-02,  5.3353e-02, -2.9422e-02, -1.1095e-02, -9.7853e-03,\n",
      "         2.3128e-02,  6.7624e-02, -1.0541e-02, -2.7242e-02, -4.4043e-02,\n",
      "        -1.2116e-02,  6.7790e-03,  4.5606e-03, -2.5383e-02, -2.5471e-02,\n",
      "        -5.9219e-02,  3.5133e-02, -3.2388e-02, -9.7870e-03,  2.9751e-02,\n",
      "         2.2225e-02, -3.6098e-02,  1.0148e-02, -1.0958e-03, -7.3015e-03,\n",
      "        -1.8712e-02,  2.7463e-02,  3.8055e-02, -4.3763e-03,  1.7322e-02,\n",
      "         7.2070e-03, -3.1665e-02, -9.0102e-03, -2.4209e-02, -1.2400e-02,\n",
      "         5.3172e-02,  3.4148e-02, -1.6076e-02,  1.6635e-02,  2.8791e-02,\n",
      "         8.2132e-02, -3.1306e-02, -1.1930e-02, -4.3205e-02, -1.6389e-02,\n",
      "        -2.0060e-02,  2.5761e-02,  4.0735e-02, -1.5025e-02,  3.3193e-02,\n",
      "         1.1091e-02,  3.3729e-02,  1.0132e-02, -1.7740e-02,  4.5612e-02,\n",
      "        -3.4810e-03, -2.2312e-02,  1.4787e-02, -1.7899e-02, -1.0382e-02,\n",
      "         8.1198e-02, -1.9500e-02, -1.2085e-02, -2.2571e-02, -4.6772e-03,\n",
      "         7.1608e-02, -3.7980e-02,  1.0813e-02, -1.4723e-02,  3.5476e-03,\n",
      "        -8.9173e-03, -9.5062e-05,  5.1758e-03,  1.5546e-02,  1.3694e-02,\n",
      "         1.2857e-02, -3.6896e-02, -2.0403e-02,  1.6667e-02, -6.9200e-02,\n",
      "        -2.7030e-04,  3.4320e-02, -3.7224e-02, -1.9865e-02, -9.5018e-03,\n",
      "        -3.4895e-03,  2.5422e-02, -1.4594e-02,  2.0393e-02,  1.5666e-02,\n",
      "        -4.1572e-02, -9.6657e-03,  2.3334e-02,  1.6966e-02, -7.1294e-03,\n",
      "        -6.4230e-02, -2.0325e-02,  8.4561e-04, -1.2127e-02,  4.2438e-02,\n",
      "         5.3766e-02, -3.4059e-02, -5.6526e-02,  9.4681e-03, -1.8982e-02,\n",
      "         1.9454e-04,  1.6017e-02, -2.1198e-02,  2.3891e-02, -1.7266e-02,\n",
      "         4.6281e-02,  7.5864e-03, -3.7733e-03,  7.2326e-03, -1.3307e-02,\n",
      "         5.2062e-02,  1.7786e-02, -6.7060e-02,  2.5864e-02,  4.3617e-02,\n",
      "         3.6204e-03,  1.3648e-02,  2.9066e-02,  2.4665e-02, -1.0474e-02,\n",
      "         1.5803e-02, -1.6530e-04, -1.0631e-02, -1.7819e-02,  4.6063e-03,\n",
      "         6.8539e-03,  9.2186e-03, -2.4126e-02, -3.9429e-02,  4.6519e-03,\n",
      "        -3.5157e-02, -2.0254e-02, -1.3913e-02, -2.0034e-02,  2.8131e-02,\n",
      "        -1.1438e-03, -7.2759e-03, -7.5065e-03,  5.0456e-03,  1.4963e-02,\n",
      "        -2.6037e-02,  1.4312e-03,  1.8442e-02,  4.1285e-03,  2.0824e-02,\n",
      "         5.5277e-02,  3.7604e-02, -2.5134e-03,  8.7348e-03, -1.1006e-02,\n",
      "        -2.0686e-02,  3.4892e-02,  2.0180e-02,  2.1242e-02, -5.8049e-02,\n",
      "        -7.9544e-03,  1.2430e-02, -5.0636e-03,  2.0754e-02,  3.9245e-02,\n",
      "         3.1034e-02, -6.3552e-03,  4.9251e-02, -1.0328e-02, -1.9130e-03,\n",
      "         1.6653e-03,  4.6319e-02,  6.7160e-02,  4.9437e-02,  1.0974e-02,\n",
      "         4.0955e-02,  4.7332e-02, -3.1411e-02,  1.9038e-02,  1.4016e-03,\n",
      "         8.3786e-04, -1.3167e-03, -3.0060e-02,  3.5156e-02, -3.6966e-02,\n",
      "        -3.3495e-02, -2.0814e-04,  1.8036e-02, -3.1302e-02, -1.5563e-02,\n",
      "        -2.4170e-02,  1.8217e-02,  4.3525e-02,  6.8470e-02,  2.1674e-03,\n",
      "        -5.0249e-02,  7.8559e-03,  6.1723e-02, -2.0784e-02,  3.3256e-02,\n",
      "        -3.3239e-02,  2.5741e-02,  1.5859e-02,  1.3865e-02, -2.5209e-02,\n",
      "        -4.1543e-03, -4.7291e-02,  7.2611e-02, -1.0510e-02,  1.7511e-05,\n",
      "        -6.5358e-03, -9.4432e-03, -1.5745e-03, -2.4434e-02, -4.8030e-02,\n",
      "         3.3074e-02,  3.8105e-02, -4.1315e-02,  1.7002e-03,  1.9009e-02,\n",
      "         2.4376e-02,  4.1798e-03, -1.9575e-02,  2.8037e-02,  2.5910e-02,\n",
      "         6.5768e-03,  8.5127e-03,  2.6651e-02, -3.4326e-02,  1.7994e-02,\n",
      "        -1.7114e-02, -1.6916e-02, -2.4724e-02, -6.3689e-02, -2.4257e-02,\n",
      "         8.2066e-03,  5.9261e-02, -1.5663e-02, -1.1917e-02, -3.7470e-02,\n",
      "         3.1727e-03,  1.9174e-02,  7.3133e-03, -2.2339e-02,  4.4632e-02,\n",
      "         2.3571e-02, -3.1169e-03,  3.6760e-02,  2.2001e-02,  2.8517e-02,\n",
      "         4.7554e-02, -2.2660e-03, -2.9162e-02,  1.0340e-02, -3.0866e-02,\n",
      "        -6.6560e-02, -5.3167e-02, -2.3423e-02, -4.8052e-04, -1.9800e-02,\n",
      "        -3.0557e-02,  4.0461e-02, -4.6597e-02, -2.9286e-02,  1.6878e-02,\n",
      "        -3.3081e-02,  7.4256e-03,  2.0036e-02,  4.7564e-02,  2.8786e-02,\n",
      "        -3.3393e-03,  4.9544e-03, -4.9661e-03,  5.3438e-02, -4.3327e-02,\n",
      "        -2.4092e-02,  1.5172e-02, -2.2931e-02,  1.0515e-02,  9.9563e-03,\n",
      "        -4.7854e-02,  3.2918e-02, -5.9170e-03,  1.8559e-02,  1.1396e-02,\n",
      "        -4.3030e-02, -3.8256e-02, -1.3744e-02, -2.1957e-02,  2.0666e-02,\n",
      "        -7.8658e-05,  8.1679e-03,  1.0702e-02,  1.9070e-02,  3.7444e-02,\n",
      "         4.0930e-02,  2.1703e-02,  2.2303e-02,  2.3048e-02,  3.6076e-02,\n",
      "        -1.5455e-02,  3.5877e-02,  7.5303e-03,  3.3510e-02,  4.0804e-02,\n",
      "         4.1558e-02, -1.1144e-02, -2.9076e-03,  1.3938e-03,  2.3249e-02,\n",
      "         2.0598e-02, -5.7033e-02, -7.4923e-03,  7.4696e-02, -1.8405e-02,\n",
      "         1.1709e-02,  1.0595e-02,  9.8685e-03,  1.5368e-02,  1.9245e-02,\n",
      "         7.3721e-03,  3.0538e-02,  1.8355e-02, -2.8027e-02, -1.2983e-02,\n",
      "        -1.1340e-02,  6.8099e-03, -5.7146e-02, -3.6839e-02,  8.1320e-03,\n",
      "         2.2358e-02,  5.7015e-03,  6.5350e-02, -1.3341e-02, -4.5012e-03,\n",
      "        -3.6502e-02,  1.6582e-02, -1.1105e-02,  4.7336e-02,  1.5099e-02,\n",
      "         6.1993e-02,  2.7813e-02, -2.5535e-02,  1.9852e-02,  1.3732e-03,\n",
      "         3.8510e-02, -2.5174e-02, -2.4682e-02, -1.1815e-02,  1.7641e-02,\n",
      "         1.5675e-02,  2.4449e-02, -3.6264e-02, -5.9098e-02, -2.9421e-02,\n",
      "         5.1559e-02,  2.7603e-02, -2.8536e-02, -2.1285e-03, -4.8935e-02,\n",
      "         1.6608e-02, -2.4604e-03,  2.3181e-04, -6.8029e-03,  1.2874e-02,\n",
      "        -2.3327e-03, -1.3326e-02,  6.6514e-03,  2.1251e-02, -3.5254e-02,\n",
      "         1.4719e-02, -1.5100e-02,  5.5907e-02,  5.8582e-03,  3.1453e-02,\n",
      "        -2.2247e-02,  3.3000e-02,  1.9160e-02, -4.0870e-02, -3.1665e-03,\n",
      "         1.6825e-02,  8.1128e-03, -9.0974e-03,  2.0807e-02, -2.5960e-03,\n",
      "        -1.6551e-02,  2.1588e-02, -9.3987e-03,  3.7023e-02, -2.2049e-03,\n",
      "         1.2642e-02,  4.6982e-02, -2.0150e-02, -3.3999e-02,  4.5824e-02,\n",
      "         7.0612e-02, -4.2149e-02,  2.3563e-02, -1.4773e-03, -8.8917e-04,\n",
      "        -4.9200e-03,  8.3329e-03, -1.9366e-02,  1.7245e-02,  8.0167e-03,\n",
      "        -4.2969e-02, -7.2973e-03, -2.7589e-02,  1.6941e-02,  4.6014e-02,\n",
      "         1.1065e-02,  8.9762e-03,  4.1690e-04, -4.8391e-02,  2.1651e-02,\n",
      "        -2.0628e-02, -2.6545e-02, -4.5530e-02, -2.1922e-02,  1.5589e-02,\n",
      "         3.4567e-02,  1.1733e-02,  2.5005e-03, -1.2395e-02,  3.1545e-03,\n",
      "        -9.9262e-03,  1.3911e-02,  5.4337e-02, -4.9246e-03, -1.4257e-02,\n",
      "         2.6233e-02,  8.0975e-03,  9.3670e-03,  5.5773e-02, -6.3219e-02,\n",
      "         4.7817e-02, -1.3402e-02,  2.0314e-03,  1.9360e-02, -2.1045e-02,\n",
      "        -9.7023e-03, -1.7401e-02, -7.5530e-02,  2.6933e-02,  5.7001e-02,\n",
      "        -1.8884e-02, -3.0144e-02, -1.8512e-03,  4.8309e-02, -6.7785e-03,\n",
      "        -4.1327e-03,  6.2755e-03,  7.4424e-03,  1.6767e-02,  3.3261e-02,\n",
      "         3.4142e-02, -3.3758e-02,  2.4977e-02,  2.8663e-02,  1.1481e-02,\n",
      "        -3.8502e-03, -9.5295e-03, -2.5840e-02,  1.7831e-02,  3.0694e-02,\n",
      "        -6.3278e-02,  1.4604e-02, -3.6883e-02, -2.4777e-02, -1.7092e-02,\n",
      "        -3.4407e-02, -2.2817e-02,  3.1009e-02, -1.5351e-02, -3.0136e-02,\n",
      "         1.5572e-02, -2.1044e-02,  5.7222e-02, -2.2117e-02,  6.1432e-03,\n",
      "        -3.1170e-02,  6.4252e-03, -3.7271e-02, -6.1883e-02, -5.0516e-02,\n",
      "        -8.8097e-03,  1.4754e-02,  5.6224e-02, -3.9936e-02, -3.2076e-02,\n",
      "        -7.1240e-03,  2.2728e-02,  1.4145e-02,  4.9475e-02,  4.4143e-02,\n",
      "         4.3013e-02,  2.6601e-02,  1.0734e-02, -4.7355e-02,  2.4956e-02,\n",
      "         2.7162e-02,  8.8756e-03,  4.0817e-02, -6.0928e-02,  7.0916e-03,\n",
      "         3.0352e-03, -1.1555e-03, -2.0933e-02,  3.9401e-02, -4.0996e-02,\n",
      "        -2.3466e-02,  2.5773e-02,  2.0778e-02, -1.8383e-02, -1.6875e-02,\n",
      "        -1.1972e-02,  1.4231e-02,  2.3394e-03,  2.8423e-03,  7.1193e-03,\n",
      "         1.3313e-02,  2.7721e-02,  2.9086e-02,  6.1350e-02,  1.6725e-02,\n",
      "         1.2422e-02,  1.8542e-02,  1.0296e-02,  2.3991e-02, -1.5753e-02,\n",
      "        -6.7098e-03, -3.9340e-02, -2.3524e-02,  6.4797e-03, -3.8882e-02,\n",
      "        -2.6252e-02,  1.9769e-02,  1.6648e-02, -1.6595e-02, -2.7671e-02,\n",
      "         2.1272e-02, -1.2319e-02,  4.2085e-02, -3.9615e-02, -3.4059e-02,\n",
      "         9.9948e-03,  4.3607e-02, -7.6010e-03,  3.5367e-02, -1.4131e-03,\n",
      "         4.3853e-02,  6.8329e-02, -7.2220e-03,  2.1834e-02, -1.7048e-02,\n",
      "        -2.5219e-02, -1.1259e-03, -1.0235e-02, -1.5787e-02, -3.0919e-02,\n",
      "         1.2026e-02,  2.9195e-02, -1.9200e-02,  1.8494e-02,  3.2172e-02,\n",
      "         1.4308e-02,  2.9589e-02,  2.9128e-03, -3.3985e-02,  1.5061e-02,\n",
      "        -2.4958e-02, -1.8519e-02, -1.7217e-02, -5.9759e-02, -2.8252e-02,\n",
      "         1.9212e-03, -3.7634e-02,  8.9987e-03,  1.8950e-02, -4.8034e-02,\n",
      "         8.4422e-02, -5.1684e-02,  2.1661e-02, -2.2183e-02, -1.1368e-02,\n",
      "        -7.1392e-02, -3.0609e-02,  3.1077e-02,  1.5938e-02,  1.7115e-02,\n",
      "         7.2417e-02, -8.0210e-02, -2.4844e-02, -1.7636e-02,  6.4550e-02,\n",
      "         4.9830e-03,  4.2789e-02,  1.0572e-02, -3.8701e-02,  5.4020e-02,\n",
      "         1.7259e-02, -1.4976e-02,  6.4248e-02,  2.5562e-02,  2.0005e-02,\n",
      "         1.1767e-02, -4.0391e-02,  4.4028e-02, -1.4923e-02, -4.8239e-02,\n",
      "         1.1739e-02,  4.1315e-02,  6.1511e-03, -3.5005e-02, -4.7347e-02,\n",
      "        -2.0855e-02,  7.3853e-02,  4.4461e-02, -1.9355e-02,  2.9831e-02,\n",
      "        -3.8765e-02, -1.4384e-02,  3.2913e-02, -3.9838e-02,  1.5641e-02,\n",
      "         2.3347e-02,  3.1875e-02, -1.8037e-02, -3.7963e-02,  4.3429e-02,\n",
      "         2.1682e-02, -1.4383e-02, -1.6270e-02,  2.7285e-03,  3.2534e-02,\n",
      "         3.8809e-03, -4.4435e-02, -2.7491e-02,  1.4417e-02,  2.2243e-02,\n",
      "         1.2739e-02, -6.0333e-02, -1.4661e-03,  5.8047e-02,  1.5288e-02,\n",
      "         1.0786e-02,  4.6900e-02,  1.4912e-03, -3.8743e-02, -2.6277e-02,\n",
      "        -8.8892e-04, -2.0081e-02,  1.5951e-02, -8.0330e-03,  7.6375e-03,\n",
      "         7.5482e-02,  2.0411e-02, -4.0133e-03,  4.4097e-02, -2.0061e-03,\n",
      "         4.3384e-02,  4.0413e-02,  2.9658e-03,  1.9037e-02,  9.1410e-03,\n",
      "         2.6940e-02,  2.8491e-02, -6.3633e-03,  6.9466e-02,  7.4699e-02,\n",
      "         1.5235e-02,  1.8676e-02, -5.0757e-02, -1.7380e-02, -4.8451e-03,\n",
      "         3.7758e-02, -7.2585e-02,  2.5627e-03, -2.9573e-04, -4.2653e-03,\n",
      "        -3.9343e-02,  5.3138e-03,  2.1474e-02, -3.4863e-02, -1.6792e-02,\n",
      "         5.3342e-02,  2.4754e-02, -1.3516e-02, -2.3305e-02,  3.0907e-02,\n",
      "         3.1985e-02, -9.9603e-03, -1.9265e-02, -1.6882e-02, -1.4670e-03,\n",
      "        -4.7131e-03,  7.9718e-02,  7.8492e-03, -4.5672e-02, -6.0936e-03,\n",
      "         1.0358e-02, -1.2461e-02, -1.2255e-02, -5.6333e-03, -8.2131e-03,\n",
      "         5.0386e-02, -2.2449e-02, -2.7391e-02, -4.9200e-02,  7.4587e-02,\n",
      "        -3.1469e-02,  2.3054e-02,  3.7359e-03,  1.9179e-02, -1.6132e-02,\n",
      "        -1.2737e-02,  3.4367e-02,  2.0445e-02,  1.1880e-02, -3.0778e-02,\n",
      "        -3.3190e-03,  7.1783e-02,  3.4815e-02, -1.0749e-02,  4.8849e-02,\n",
      "        -8.0632e-03,  3.6747e-02, -1.2520e-02,  1.9272e-02, -3.0332e-03,\n",
      "         3.9912e-02,  2.3565e-02, -1.6342e-02, -4.7917e-03, -1.5781e-02,\n",
      "         1.5440e-02,  1.1500e-02, -4.1674e-02, -8.5174e-03,  1.0393e-02,\n",
      "        -7.6011e-03, -2.3717e-02,  2.2106e-02,  2.8221e-02,  5.2663e-03,\n",
      "         8.1169e-03, -1.7444e-02, -2.5450e-02, -2.0701e-03,  3.0893e-02,\n",
      "        -1.6296e-02, -4.2676e-02,  2.0419e-02,  1.2724e-02,  2.2784e-02],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eteco\\AppData\\Local\\Temp\\ipykernel_16952\\1079021095.py:13: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3687.)\n",
      "  x = (x @ self.embedding.weight).T\n"
     ]
    }
   ],
   "source": [
    "# Test the whole thing\n",
    "config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "transformer = Transformer(num_blocks=2, config=config)\n",
    "x = torch.ones(config.d_vocab, dtype=torch.float)\n",
    "y: Float[torch.Tensor, \"vocab seq_len\"] = transformer(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f7f1",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cedf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: Transformer,\n",
    "    loss: torch.nn.CrossEntropyLoss = nn.CrossEntropyLoss(),\n",
    "    lr: Float = 1e-3,\n",
    "    epochs: Int = 1\n",
    "    ):\n",
    "    optimizer: torch.optim.SGD = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    string_data = get_many_books(ids=range(10,15), data_temp=\"./data/gutenberg_data\")\n",
    "    tokenized_string_data = [x for i in string_data[]]\n",
    "    labels = {}\n",
    "    int_labels = []\n",
    "    for i in range(len(string_data)):\n",
    "        labels[i] = string_data[i]\n",
    "        int_labels.append(i)\n",
    "    \n",
    "    training_data = torch.tensor(int_labels)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(training_data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89cd1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 10...\n",
      "\t4432261 characters read\n",
      "Getting book 11...\n",
      "\t148062 characters read\n",
      "Getting book 12...\n",
      "\t168390 characters read\n",
      "Getting book 13...\n",
      "\t34579 characters read\n",
      "Getting book 14...\n",
      "\t1951150 characters read\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config: Config \u001b[38;5;241m=\u001b[39m Config(d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, d_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, d_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(num_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, loss, lr, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab seq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m     12\u001b[0m         x \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\llm_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "config: Config = Config(d_model=16, d_vocab=1000, d_hidden=64)\n",
    "model = Transformer(num_blocks=2, config=config)\n",
    "train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding-gpt (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
